{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7bff3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "import random\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "ca302927",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMWithLMHeadModel were not initialized from the model checkpoint at xlm-mlm-enfr-1024 and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMTokenizer, XLMWithLMHeadModel\n",
    "\n",
    "model_checkpoint = 'xlm-mlm-enfr-1024'\n",
    "tokenizer = XLMTokenizer.from_pretrained(model_checkpoint)\n",
    "pre_model = XLMWithLMHeadModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a051419d",
   "metadata": {},
   "source": [
    "# Load data here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f01ea585",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en = open(\"data/hansards.e\", encoding='utf-8').read().split('\\n')\n",
    "data_fr = open(\"data/hansards.f\", encoding='utf-8').read().split('\\n')\n",
    "\n",
    "\n",
    "raw_data_en = {'en': [line for line in data_en]}\n",
    "raw_data_fr = {'fr': [line for line in data_fr]}\n",
    "\n",
    "df_en = pd.DataFrame(raw_data_en, columns = ['en'])\n",
    "df_fr = pd.DataFrame(raw_data_fr, columns = ['fr'])\n",
    "\n",
    "df_small = df[['en', 'fr']][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfca6e6",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b7ac83db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Tokenizer\n",
    "#model_checkpoint = 't5-small'\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, model_max_length=512)\n",
    "\n",
    "# Function for mapping data from strings to tokens\n",
    "# s_key = source key, t_key = target_key, max_length = max sentence length\n",
    "def preprocess_data(df, s_key, t_key, max_length):\n",
    "    s = [sentence for sentence in df[s_key]]\n",
    "    t = [sentence for sentence in df[t_key]]\n",
    "    \n",
    "    model_input = tokenizer(s, max_length=max_length, truncation=True, padding=True, return_tensors='pt') \n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_tokens = tokenizer(t, truncation=True, padding=True, max_length=max_length, return_tensors='pt') \n",
    "        \n",
    "    model_input['target'] = target_tokens['input_ids']\n",
    "   \n",
    "    return model_input\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_data(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_token_type_ids=False,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return enc_di\n",
    "\n",
    "\n",
    "# Tokenize data\n",
    "en_data = tokenize_data(df_small['en'], tokenizer)\n",
    "fr_data = tokenize_data(df_small['fr'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "64343ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of tokenize output\n",
    "#print(en_data['input_ids'][1])\n",
    "#print(df_small['en'][1]) # \"the\" occures on idx 0\n",
    "#print(df_small['en'][2]) # \"the\" occures on idx 13\n",
    "#print(en_data['input_ids'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7cccc1",
   "metadata": {},
   "source": [
    "## All transformer classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2b4e94d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Allocate memory to \n",
    "        pe = torch.zeros((max_seq_len, d_model))\n",
    "        \n",
    "        ### From attention is all you need ###\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0,d_model,2):\n",
    "                pe[pos, i] = np.sin(pos/10000**(2*i/self.d_model))\n",
    "                pe[pos, i+1] = np.cos(pos/10000**(2*i/self.d_model))\n",
    "        # Fixed positional encoding\n",
    "        pe.requires_grad = False\n",
    "        #pe = pe.unsqueeze(0) # Make pe into [batch size x seq_len x d_model]\n",
    "        self.register_buffer('pe',pe)\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        # Make embeddings larger\n",
    "        x = x*np.sqrt(self.d_model)\n",
    "        # Get sequence length\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        pe = self.pe.clone()\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        x = x + torch.autograd.Variable(pe[:,:seq_len], \n",
    "        requires_grad=False)\n",
    "        return x\n",
    "\n",
    "\n",
    "def Attention(Q, K, V, d_k, mask=None, dropout=None):\n",
    "\n",
    "    vals = (Q @ K.transpose(-2,-1))/np.sqrt(d_k)\n",
    "    \n",
    "    # Mask the scores if mask is specified. Model cannot see into future if masked.\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        vals = vals.masked_fill(mask, -1000) # changed from 1e-4 \n",
    "    # vals = vals if mask is None else vals.masked_fill_(mask, 1e-4)\n",
    "    \n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    vals = softmax(vals)\n",
    "    \n",
    "    # apply dropout if specified\n",
    "    vals = vals if dropout is None else dropout(vals)\n",
    "    \n",
    "    out =  vals @ V\n",
    "    return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_k, dropout=.1, relative = False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        # self.seq_len = seq_len\n",
    "        self.d_k = d_k\n",
    "        \n",
    "        self.linearQ = nn.Linear(d_model, d_model)\n",
    "        self.linearK = nn.Linear(d_model, d_model)\n",
    "        self.linearV = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    # d_model = 512\n",
    "    # n_heads = 8\n",
    "    # seq_len = 20\n",
    "    \n",
    "    # [20,512] --> [20, 8, 64]\n",
    "    ## If batch size is used, say of 128:\n",
    "    ## out = [128, 20, 8, 64]\n",
    "    \n",
    "    # Input = Matrix of dim [bs x seq_len x d_model]\n",
    "    def split_heads(self, t):\n",
    "        return t.reshape(t.size(0), -1, self.n_heads, int(self.d_k))\n",
    "    # Output = Matrix of dim [bs x seq_len x n_heads x d_k]\n",
    "    \n",
    "    def forward(self, Q, K, V, mask = None):\n",
    "        \n",
    "        Q = self.linearQ(Q)\n",
    "        K = self.linearK(K)\n",
    "        V = self.linearV(V)\n",
    "        \n",
    "        Q, K, V = [self.split_heads(t) for t in (Q, K, V)] \n",
    "        Q, K, V = [t.transpose(1,2) for t in (Q, K, V)] # reshape to [bs x n_heads x seq_len x d_k]\n",
    "        \n",
    "        # Compute Attention\n",
    "        vals = Attention(Q, K, V, self.d_k, mask, self.dropout)\n",
    "        \n",
    "        # Reshape to [bs x seq_len x d_model]\n",
    "        vals = vals.transpose(1,2).contiguous().view(vals.size(0), -1, self.d_model)\n",
    "       \n",
    "        out = self.out(vals) # linear\n",
    "        return out\n",
    "    \n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "    \n",
    "\n",
    "        \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_ff, d_k, dropout=.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(n_heads, d_model, d_k, dropout)\n",
    "        self.ffns = FeedForwardNetwork(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # See \"Attention is all you need\" to follow code structure\n",
    "        x2 = self.dropout1(self.attention(x, x, x, mask))\n",
    "        x = self.layer_norm1(x) + self.layer_norm1(x2)\n",
    "        \n",
    "        x2 = self.dropout2(self.ffns(x))\n",
    "        x = x + self.layer_norm2(x2)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, d_ff, d_k, dropout=.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.attention = MultiHeadAttention(n_heads, d_model, d_k, dropout)\n",
    "        self.ffns = FeedForwardNetwork(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Batch Normalization\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "        # self.linear = nn.Linear()\n",
    "        \n",
    "    def forward(self, x, e_out, source_mask, target_mask):\n",
    "        \n",
    "        # See \"Attention is all you need\" to follow code structure\n",
    "        ## part 1\n",
    "        x2 = self.layer_norm1(x) # Norm\n",
    "        x = self.dropout1(self.attention.forward(x2, x2, x2, target_mask)) # Masked MHA, target\n",
    "        x = x2 + self.layer_norm1(x) # Add & Norm\n",
    "        \n",
    "        ## part 2\n",
    "        x3 = self.dropout2(self.attention.forward(x, e_out, e_out, source_mask)) # MHA on encoder output\n",
    "        x2 = self.dropout2(self.attention.forward(x, x, x)) #MHA continued in decoder\n",
    "        x = self.layer_norm1(x3) + self.layer_norm1(x2) + self.layer_norm1(x) # Add & Norm\n",
    "        \n",
    "        ## part 3\n",
    "        x2 = self.dropout3(self.ffns.forward(x)) ## Feed forward\n",
    "        x = x + self.layer_norm2(x2) # add\n",
    "        # x = self.norm3(x) # norm (!!!CHECK IF THIS IS EQUIVALENT!!!)\n",
    "        return x\n",
    "\n",
    "def cloneLayers(module, n_layers):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for i in range(n_layers)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_ff, d_k, n_layers, n_heads, dropout=.1):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.embedder = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.e_layers = cloneLayers(EncoderLayer(n_heads, d_model, d_ff, d_k), n_layers)\n",
    "        \n",
    "    def forward(self, source, mask=None):\n",
    "        x = self.embedder.forward(source)\n",
    "        x = self.pe.forward(x)\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.e_layers[i](x, mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_ff, d_k, n_layers, n_heads, dropout=.1):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.embedder = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.d_layers = cloneLayers(DecoderLayer(n_heads, d_model, d_ff, d_k), n_layers)\n",
    "        \n",
    "    \n",
    "    def forward(self, trg, e_out, source_mask, target_mask):\n",
    "        x = self.embedder.forward(trg)\n",
    "        x = self.pe.forward(x)\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            x = self.d_layers[i](x, e_out, source_mask, target_mask)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "class AlignmentLayer(nn.Module):\n",
    "    def __init__(self, source_vocab_size, target_vocab_size, d_model, d_ff, d_k, n_layers, n_heads):\n",
    "        super().__init__()\n",
    "        self.MHA = MultiHeadAttention(n_heads, d_model, d_k)\n",
    "        self.e = Encoder(source_vocab_size, d_model,d_ff, d_k, n_layers, n_heads)\n",
    "        self.d = Decoder(target_vocab_size, d_model,d_ff, d_k, n_layers, n_heads)\n",
    "        self.linear_f = nn.Linear(d_model, target_vocab_size) \n",
    "        \n",
    "    def forward(self, source, target, source_mask, target_mask):\n",
    "        e_out = self.e.forward(source, source_mask)\n",
    "        d_out = self.d.forward(target, e_out, source_mask, target_mask)\n",
    "        \n",
    "        out = self.MHA.forward(d_out, e_out, e_out)\n",
    "        \n",
    "        \n",
    "        \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, source_vocab_size, target_vocab_size, d_model,d_ff, d_k, n_layers, n_heads):\n",
    "        super().__init__()\n",
    "        self.e = Encoder(source_vocab_size, d_model,d_ff, d_k, n_layers, n_heads)\n",
    "        self.d = Decoder(target_vocab_size, d_model,d_ff, d_k, n_layers, n_heads)\n",
    "        self.linear_f = nn.Linear(d_model, target_vocab_size)\n",
    "        \n",
    "    def forward(self, source, target, source_mask, target_mask):\n",
    "        e_out = self.e.forward(source, source_mask)\n",
    "        d_out = self.d.forward(target, e_out, source_mask, target_mask)\n",
    "        \n",
    "        out = self.linear_f(d_out)\n",
    "        return out\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f1e3f488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_mask(size, target):\n",
    "    mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float(0)).masked_fill(mask == 1, float(1))\n",
    "    \n",
    "    # Find out where the target pad starts\n",
    "    trg_pad = (target==0).nonzero()\n",
    "    \n",
    "    # Check if there is no padding in sentence\n",
    "    if len(trg_pad) == 0:\n",
    "        stop_idx = size\n",
    "        \n",
    "    else:\n",
    "        stop_idx = trg_pad[0][1].item()\n",
    "        mask[stop_idx:, :] = -69\n",
    "        \n",
    "    return mask.unsqueeze(0) > 0, stop_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3e7abf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_mask(size, source):\n",
    "    src_pad = (source==2).nonzero()\n",
    "    \n",
    "    if len(src_pad) == 0:\n",
    "        stop_idx = size\n",
    "        \n",
    "    else:\n",
    "        stop_idx = src_pad[0][1].item()\n",
    "        \n",
    "    mask = source.clone()\n",
    "    # Mask all padding with -inf\n",
    "    mask[:,stop_idx:] = 1\n",
    "    # Convert everything before stop_idx to zero\n",
    "    mask[:,:stop_idx] = 0\n",
    "    \n",
    "    mask = mask.unsqueeze(0) > 0\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1b1aeaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Testing masking function for new tokenizer...\")\n",
    "\n",
    "#en1 = en_data['input_ids'][1]\n",
    "#en1.size()\n",
    "#pad = (en1==2).nonzero()\n",
    "#pad[0].item()\n",
    "#en1_msk = get_source_mask(en1.size(), en1.unsqueeze(0))\n",
    "#en1_new = en1\n",
    "\n",
    "\n",
    "#print(\"source:\\n\", en1)\n",
    "#en1_new = en1_new.masked_fill(en1_msk, 1e-5)\n",
    "#print(\"mask:\\n\", en1_msk)\n",
    "#en1_new = en1_new.masked_fill(en1_msk, -1)\n",
    "#print(\"after masking:\\n\", en1_new)\n",
    "\n",
    "\n",
    "#fr1 = fr_data['input_ids'][1]\n",
    "#fr1_msk = get_source_mask(fr1.size(), fr1.unsqueeze(0))\n",
    "#fr1_new = fr1\n",
    "\n",
    "#fr1_new = fr1_new.masked_fill(fr1_msk, -1)\n",
    "#fr1_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32473e87",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Define train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "0910e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 10\n",
    "def train_model(model, source_data, target_data, epochs, verbose=False):\n",
    "    print(\"Training transformer\")\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    loss_list = []\n",
    "    \n",
    "    source_all = source_data['input_ids']\n",
    "    target_all = target_data['input_ids']\n",
    "    \n",
    "    # loop over epochs\n",
    "    count = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch\", epoch+1)\n",
    "        # loop over all sentences\n",
    "        for i in range(len(source_all)):\n",
    "            if verbose and i % print_every == 0:\n",
    "                print(\"sentece\", i)\n",
    "            \n",
    "            # unsqueeze to avoid dim mismatch between embedder and pe\n",
    "            src = source_all[i].unsqueeze(0) \n",
    "            trg = target_all[i].unsqueeze(0)\n",
    "            \n",
    "            # target input, remove last word\n",
    "            trg_input = trg[:, :-1]\n",
    "            \n",
    "            # get targets\n",
    "            y = trg[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            src_mask = get_source_mask(src.size(1), src)\n",
    "            trg_mask, trg_stop_idx = get_target_mask(trg_input.size(1), trg_input)\n",
    "            \n",
    "            preds = model.forward(src, trg_input, src_mask, trg_mask)\n",
    "            optim.zero_grad()    \n",
    "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), y)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.item()\n",
    "            loss_list.append(loss.item())\n",
    "            \n",
    "            count += 1\n",
    "            if verbose and i % print_every == 0:\n",
    "                print(\"time:\",np.round(time.time()-start, 2), \"\\navg loss:\", np.round(total_loss/count, 2), \"\\ntotal loss:\", np.round(total_loss,2)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "695fb80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!REMEMBER TO CHANGE EPOCHS!!!\n"
     ]
    }
   ],
   "source": [
    "### Define arguments ### (same as in \"Attention is all you need\")\n",
    "d_model = 1024 # Dimension of embeddings\n",
    "n_heads = 8 # Number of heads for MHA\n",
    "d_k = d_model/n_heads # dimension of keys (d_model / n_heads)\n",
    "d_ff = d_model*4 # DON'T CHANGE!!! (be careful)\n",
    "vocab_size = len(df) # Number of (unique) words in dataset\n",
    "n_layers = 6 # Number of model layers\n",
    "epochs = 1\n",
    "\n",
    "\n",
    "print(\"!!!REMEMBER TO CHANGE EPOCHS!!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc69479c",
   "metadata": {},
   "source": [
    "# Define model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "bb6d8054",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_vocab_size = 64139\n",
    "model = Transformer(pre_vocab_size, pre_vocab_size, d_model, d_ff, d_k, n_layers, n_heads)\n",
    "\n",
    "lr = 0.00001 # 0.0001 default in \"AIAYN\"\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0117ad93",
   "metadata": {},
   "source": [
    "# Get pre-trained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4d5b50bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(\"before weight initialization:\")\n",
    "#print(model.state_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "172013d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_model_weights = pre_model.state_dict()\n",
    "sd = model.state_dict().copy()\n",
    "model_weights = model.state_dict()\n",
    "\n",
    "count = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for j in ['e.e_layers', 'd.d_layers']:\n",
    "        for i in range(n_layers):\n",
    "            # Embedding and Positional Encoding\n",
    "            sd['e.embedder.embed.weight'] = pre_model_weights['transformer.embeddings.weight']\n",
    "            sd['e.pe.pe'] = pre_model_weights['transformer.position_embeddings.weight']\n",
    "\n",
    "            # Attention layers\n",
    "            sd[f'{j}.{i}.attention.linearQ.weight']  = pre_model_weights[f'transformer.attentions.{i}.q_lin.weight']\n",
    "            sd[f'{j}.{i}.attention.linearQ.bias']    = pre_model_weights[f'transformer.attentions.{i}.q_lin.bias']\n",
    "            sd[f'{j}.{i}.attention.linearK.weight']  = pre_model_weights[f'transformer.attentions.{i}.k_lin.weight']\n",
    "            sd[f'{j}.{i}.attention.linearK.bias']    = pre_model_weights[f'transformer.attentions.{i}.k_lin.bias']\n",
    "            sd[f'{j}.{i}.attention.linearV.weight']  = pre_model_weights[f'transformer.attentions.{i}.v_lin.weight']\n",
    "            sd[f'{j}.{i}.attention.linearV.bias']    = pre_model_weights[f'transformer.attentions.{i}.v_lin.bias']\n",
    "            sd[f'{j}.{i}.attention.out.weight']   = pre_model_weights[f'transformer.attentions.{i}.out_lin.weight']\n",
    "            sd[f'{j}.{i}.attention.out.bias']     = pre_model_weights[f'transformer.attentions.{i}.out_lin.bias']\n",
    "\n",
    "            # Feed forwards\n",
    "            sd[f'{j}.{i}.ffns.linear1.weight'] = pre_model_weights[f'transformer.ffns.{i}.lin1.weight']\n",
    "            sd[f'{j}.{i}.ffns.linear1.bias'] = pre_model_weights[f'transformer.ffns.{i}.lin1.bias']\n",
    "            sd[f'{j}.{i}.ffns.linear2.weight'] = pre_model_weights[f'transformer.ffns.{i}.lin2.weight']\n",
    "            sd[f'{j}.{i}.ffns.linear2.bias'] = pre_model_weights[f'transformer.ffns.{i}.lin2.bias']\n",
    "\n",
    "            # Layer_norm1 = attention\n",
    "            sd[f'{j}.{i}.layer_norm1.weight'] = pre_model_weights[f'transformer.layer_norm1.{i}.weight']\n",
    "            sd[f'{j}.{i}.layer_norm1.bias'] = pre_model_weights[f'transformer.layer_norm1.{i}.bias']\n",
    "\n",
    "            #Layer_norm2 = FFN\n",
    "            sd[f'{j}.{i}.layer_norm2.weight'] = pre_model_weights[f'transformer.layer_norm2.{i}.weight']\n",
    "            sd[f'{j}.{i}.layer_norm2.bias'] = pre_model_weights[f'transformer.layer_norm2.{i}.bias']\n",
    "\n",
    "            # prediction layer\n",
    "            sd['linear_f.weight'] = pre_model_weights['pred_layer.proj.weight']\n",
    "            sd['linear_f.bias'] = pre_model_weights['pred_layer.proj.bias']\n",
    "            \n",
    "            \n",
    "            # fix\n",
    "            count += 1\n",
    "            if count >= len(list(model.state_dict().keys())):\n",
    "                break\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4bb64e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cb4beb",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "489250c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training transformer\n",
      "epoch 1\n",
      "sentece 0\n",
      "time: 14.06 \n",
      "avg loss: 78.32 \n",
      "total loss: 78.32\n",
      "sentece 10\n",
      "time: 152.0 \n",
      "avg loss: 94.6 \n",
      "total loss: 1040.56\n",
      "sentece 20\n",
      "time: 234.43 \n",
      "avg loss: 95.92 \n",
      "total loss: 2014.32\n",
      "sentece 30\n",
      "time: 303.38 \n",
      "avg loss: 84.81 \n",
      "total loss: 2629.01\n",
      "sentece 40\n",
      "time: 391.86 \n",
      "avg loss: 77.99 \n",
      "total loss: 3197.78\n",
      "sentece 50\n",
      "time: 487.13 \n",
      "avg loss: 75.98 \n",
      "total loss: 3874.77\n",
      "sentece 60\n",
      "time: 592.34 \n",
      "avg loss: 72.58 \n",
      "total loss: 4427.33\n",
      "sentece 70\n",
      "time: 684.17 \n",
      "avg loss: 67.44 \n",
      "total loss: 4788.12\n",
      "sentece 80\n",
      "time: 784.44 \n",
      "avg loss: 63.51 \n",
      "total loss: 5144.44\n",
      "sentece 90\n",
      "time: 876.19 \n",
      "avg loss: 60.35 \n",
      "total loss: 5491.66\n"
     ]
    }
   ],
   "source": [
    "train_model(model, en_data, fr_data, epochs, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf012d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'weights\\EN-FR_translation_weights.pt'\n",
    "torch.save(model.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db22fc38",
   "metadata": {},
   "source": [
    "# Get model weights manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "623ab7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights = {}\n",
    "for name, param in model.named_parameters():\n",
    "    model_weights[name] = param.detach().numpy()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "567e853d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('e.embedder.embed.weight',\n",
       "              tensor([[-0.0242, -0.0930,  0.0500,  ...,  0.1343, -0.0258, -0.0163],\n",
       "                      [-0.0104, -0.0273,  0.0954,  ...,  0.0880, -0.0083,  0.0041],\n",
       "                      [ 0.0391, -0.0052,  0.0927,  ...,  0.0465,  0.0390,  0.0123],\n",
       "                      ...,\n",
       "                      [-0.0285, -0.0183, -0.0318,  ...,  0.0191,  0.0552, -0.0374],\n",
       "                      [-0.0028,  0.0298,  0.0414,  ..., -0.0129, -0.0147, -0.0737],\n",
       "                      [ 0.0210, -0.0002,  0.0307,  ..., -0.0009,  0.0030, -0.0063]])),\n",
       "             ('e.pe.pe',\n",
       "              tensor([[-0.0242, -0.0930,  0.0500,  ...,  0.1343, -0.0258, -0.0163],\n",
       "                      [-0.0104, -0.0273,  0.0954,  ...,  0.0880, -0.0083,  0.0041],\n",
       "                      [ 0.0391, -0.0052,  0.0927,  ...,  0.0465,  0.0390,  0.0123],\n",
       "                      ...,\n",
       "                      [-0.0285, -0.0183, -0.0318,  ...,  0.0191,  0.0552, -0.0374],\n",
       "                      [-0.0028,  0.0298,  0.0414,  ..., -0.0129, -0.0147, -0.0737],\n",
       "                      [ 0.0210, -0.0002,  0.0307,  ..., -0.0009,  0.0030, -0.0063]])),\n",
       "             ('e.e_layers.0.attention.linearQ.weight',\n",
       "              tensor([[-0.0087, -0.1793,  0.0245,  ...,  0.1347, -0.0830, -0.1262],\n",
       "                      [-0.0476, -0.0668,  0.1115,  ...,  0.0627,  0.0377, -0.0942],\n",
       "                      [ 0.0377,  0.1061,  0.0560,  ..., -0.0478, -0.0238, -0.0410],\n",
       "                      ...,\n",
       "                      [-0.0754, -0.0742,  0.2679,  ...,  0.0081, -0.0088,  0.0206],\n",
       "                      [-0.0698, -0.0037, -0.0277,  ...,  0.0625,  0.0602, -0.0348],\n",
       "                      [-0.1392, -0.0295, -0.0606,  ..., -0.0315,  0.0156, -0.0347]])),\n",
       "             ('e.e_layers.0.attention.linearQ.bias',\n",
       "              tensor([0.0375, 0.3299, 0.0121,  ..., 0.0533, 0.1462, 0.1385])),\n",
       "             ('e.e_layers.0.attention.linearK.weight',\n",
       "              tensor([[ 0.0702,  0.0082, -0.0908,  ..., -0.0864,  0.0325,  0.0618],\n",
       "                      [ 0.0567, -0.0846,  0.1303,  ...,  0.0646,  0.0282,  0.0964],\n",
       "                      [ 0.0674, -0.0822, -0.0075,  ..., -0.0487,  0.0630, -0.1170],\n",
       "                      ...,\n",
       "                      [-0.0398, -0.0953,  0.0399,  ...,  0.1075,  0.0879, -0.0253],\n",
       "                      [ 0.0473,  0.0310, -0.2221,  ...,  0.0166, -0.0768,  0.0466],\n",
       "                      [ 0.0505,  0.0816,  0.1128,  ...,  0.0025, -0.0634,  0.0541]])),\n",
       "             ('e.e_layers.0.attention.linearK.bias',\n",
       "              tensor([-0.0068, -0.0168, -0.0112,  ..., -0.0190, -0.0251, -0.0015])),\n",
       "             ('e.e_layers.0.attention.linearV.weight',\n",
       "              tensor([[ 0.0040, -0.0074, -0.0066,  ..., -0.0269,  0.0299, -0.0202],\n",
       "                      [ 0.0477, -0.0207,  0.0066,  ..., -0.0238,  0.0273,  0.0186],\n",
       "                      [-0.0112,  0.0097,  0.0115,  ...,  0.0251,  0.0149,  0.0284],\n",
       "                      ...,\n",
       "                      [ 0.0343,  0.0285,  0.0078,  ...,  0.0451,  0.0414, -0.0044],\n",
       "                      [-0.0761, -0.0317,  0.0331,  ..., -0.0622, -0.0022,  0.0261],\n",
       "                      [-0.0062,  0.0272, -0.0020,  ...,  0.0136, -0.0079,  0.0159]])),\n",
       "             ('e.e_layers.0.attention.linearV.bias',\n",
       "              tensor([-0.0045, -0.0045, -0.0061,  ...,  0.0197,  0.0057,  0.0140])),\n",
       "             ('e.e_layers.0.attention.out.weight',\n",
       "              tensor([[ 0.0443,  0.0206, -0.0478,  ...,  0.0494, -0.0058, -0.0176],\n",
       "                      [ 0.0120,  0.0372,  0.0022,  ...,  0.0533,  0.0466,  0.0312],\n",
       "                      [ 0.0443, -0.0408, -0.0387,  ...,  0.0483,  0.0297, -0.0104],\n",
       "                      ...,\n",
       "                      [ 0.0102, -0.0015, -0.0344,  ...,  0.0212,  0.0401,  0.0462],\n",
       "                      [ 0.0332, -0.0142, -0.0375,  ..., -0.0534,  0.0305,  0.0408],\n",
       "                      [-0.0116, -0.0505, -0.0057,  ..., -0.0050, -0.0490,  0.0222]])),\n",
       "             ('e.e_layers.0.attention.out.bias',\n",
       "              tensor([-0.0294, -0.0241, -0.0048,  ...,  0.0231,  0.0118, -0.0042])),\n",
       "             ('e.e_layers.0.ffns.linear1.weight',\n",
       "              tensor([[-0.0245,  0.0142,  0.0445,  ..., -0.0016,  0.0058,  0.0004],\n",
       "                      [ 0.0236,  0.0234, -0.0023,  ...,  0.0327,  0.0019,  0.0490],\n",
       "                      [-0.0042,  0.0848,  0.0695,  ...,  0.0037,  0.0140, -0.0526],\n",
       "                      ...,\n",
       "                      [ 0.0148,  0.0019,  0.0427,  ...,  0.0258, -0.0592,  0.0224],\n",
       "                      [-0.0926,  0.0477,  0.0153,  ..., -0.0148,  0.0290, -0.0060],\n",
       "                      [ 0.0328, -0.0752, -0.0071,  ...,  0.0031,  0.0016, -0.0341]])),\n",
       "             ('e.e_layers.0.ffns.linear1.bias',\n",
       "              tensor([ 0.0376, -0.0361, -0.1560,  ..., -0.1348, -0.0525, -0.3136])),\n",
       "             ('e.e_layers.0.ffns.linear2.weight',\n",
       "              tensor([[-0.0141, -0.0004, -0.0115,  ..., -0.0209, -0.0351,  0.0153],\n",
       "                      [ 0.0403, -0.0246,  0.0095,  ...,  0.0053,  0.1029,  0.0395],\n",
       "                      [-0.0038, -0.0410,  0.0621,  ...,  0.0053, -0.0043, -0.1205],\n",
       "                      ...,\n",
       "                      [-0.0196,  0.0050, -0.0358,  ..., -0.0532, -0.0050,  0.0517],\n",
       "                      [ 0.0351,  0.0351,  0.0264,  ...,  0.0173, -0.0307, -0.0733],\n",
       "                      [-0.0141, -0.0012,  0.0444,  ...,  0.0169,  0.0221, -0.0776]])),\n",
       "             ('e.e_layers.0.ffns.linear2.bias',\n",
       "              tensor([-0.0982,  0.1101, -0.0966,  ..., -0.0024, -0.0465,  0.0259])),\n",
       "             ('e.e_layers.0.layer_norm1.weight',\n",
       "              tensor([0.4805, 0.4905, 0.3896,  ..., 0.5049, 0.5101, 0.5389])),\n",
       "             ('e.e_layers.0.layer_norm1.bias',\n",
       "              tensor([ 0.2052,  0.0375, -0.2659,  ...,  0.0634, -0.0294,  0.0375])),\n",
       "             ('e.e_layers.0.layer_norm2.weight',\n",
       "              tensor([0.3501, 0.3243, 0.2274,  ..., 0.3455, 0.3446, 0.3474])),\n",
       "             ('e.e_layers.0.layer_norm2.bias',\n",
       "              tensor([-0.0402, -0.0095,  0.0860,  ..., -0.0054,  0.0376, -0.0076])),\n",
       "             ('e.e_layers.1.attention.linearQ.weight',\n",
       "              tensor([[-0.0194, -0.0435,  0.0669,  ..., -0.2365,  0.0262, -0.0365],\n",
       "                      [ 0.1898, -0.0872,  0.1022,  ...,  0.0029,  0.1506, -0.0259],\n",
       "                      [-0.0008, -0.1185, -0.0657,  ..., -0.1489, -0.1620,  0.0383],\n",
       "                      ...,\n",
       "                      [ 0.1145,  0.0366,  0.0889,  ..., -0.0849, -0.0265,  0.1727],\n",
       "                      [-0.1635,  0.0250,  0.0674,  ...,  0.0459,  0.2495,  0.0445],\n",
       "                      [ 0.0664,  0.0258,  0.0069,  ..., -0.1510,  0.0592, -0.1491]])),\n",
       "             ('e.e_layers.1.attention.linearQ.bias',\n",
       "              tensor([-0.1197,  0.1806, -0.1089,  ...,  0.2793, -0.1692, -0.1596])),\n",
       "             ('e.e_layers.1.attention.linearK.weight',\n",
       "              tensor([[ 0.0730, -0.0017, -0.1235,  ..., -0.0338, -0.1131, -0.1052],\n",
       "                      [ 0.0701,  0.0164,  0.0638,  ..., -0.0539,  0.0608, -0.0020],\n",
       "                      [ 0.0193,  0.0365, -0.0459,  ...,  0.0681, -0.0525,  0.0184],\n",
       "                      ...,\n",
       "                      [ 0.0022, -0.0891, -0.0529,  ...,  0.0223, -0.0227,  0.2144],\n",
       "                      [-0.0312,  0.1790, -0.1772,  ..., -0.0517, -0.0175,  0.0993],\n",
       "                      [ 0.0397, -0.0193,  0.0153,  ..., -0.1179, -0.0214, -0.2960]])),\n",
       "             ('e.e_layers.1.attention.linearK.bias',\n",
       "              tensor([ 0.0085, -0.0012, -0.0252,  ...,  0.0337, -0.0167, -0.0273])),\n",
       "             ('e.e_layers.1.attention.linearV.weight',\n",
       "              tensor([[-0.0014,  0.0468, -0.0493,  ...,  0.0178, -0.0386,  0.0385],\n",
       "                      [-0.0134,  0.0323, -0.0327,  ...,  0.0424, -0.0623,  0.0242],\n",
       "                      [-0.0358, -0.0341,  0.0175,  ..., -0.0619,  0.0387, -0.0015],\n",
       "                      ...,\n",
       "                      [ 0.0585,  0.0038, -0.0110,  ..., -0.0278,  0.0403,  0.0297],\n",
       "                      [ 0.0382, -0.0092, -0.0093,  ..., -0.0412,  0.0033,  0.0266],\n",
       "                      [ 0.0246, -0.0461,  0.0165,  ..., -0.0615, -0.0186,  0.0643]])),\n",
       "             ('e.e_layers.1.attention.linearV.bias',\n",
       "              tensor([-0.0351,  0.0107, -0.0243,  ...,  0.0291,  0.0540, -0.0563])),\n",
       "             ('e.e_layers.1.attention.out.weight',\n",
       "              tensor([[-0.0283, -0.0245,  0.0191,  ..., -0.0467,  0.0062,  0.0395],\n",
       "                      [-0.0492, -0.0294,  0.0245,  ..., -0.0155, -0.0060,  0.0274],\n",
       "                      [ 0.0496,  0.0487, -0.0330,  ...,  0.0385, -0.0060, -0.0061],\n",
       "                      ...,\n",
       "                      [-0.0442,  0.0148,  0.0451,  ...,  0.0341,  0.0239,  0.0520],\n",
       "                      [-0.0393,  0.0262, -0.0052,  ..., -0.0512,  0.0329,  0.0048],\n",
       "                      [-0.0497, -0.0330, -0.0099,  ...,  0.0380, -0.0229, -0.0096]])),\n",
       "             ('e.e_layers.1.attention.out.bias',\n",
       "              tensor([-0.0294, -0.0241, -0.0048,  ...,  0.0231,  0.0118, -0.0042])),\n",
       "             ('e.e_layers.1.ffns.linear1.weight',\n",
       "              tensor([[ 0.0106, -0.0934,  0.0356,  ...,  0.0176,  0.0992, -0.0342],\n",
       "                      [-0.0915,  0.0141,  0.0278,  ..., -0.0225,  0.0362, -0.0019],\n",
       "                      [ 0.0457, -0.1302,  0.1214,  ..., -0.0123,  0.0467,  0.0215],\n",
       "                      ...,\n",
       "                      [ 0.0847,  0.0345, -0.0131,  ..., -0.0674, -0.0047, -0.0068],\n",
       "                      [-0.0370, -0.0043, -0.0413,  ..., -0.0587, -0.0135, -0.0305],\n",
       "                      [-0.1740, -0.0796, -0.0050,  ...,  0.0170,  0.0231, -0.0087]])),\n",
       "             ('e.e_layers.1.ffns.linear1.bias',\n",
       "              tensor([-0.0918, -0.0677, -0.0687,  ...,  0.0231, -0.3387, -0.1482])),\n",
       "             ('e.e_layers.1.ffns.linear2.weight',\n",
       "              tensor([[-0.0088,  0.0246, -0.0893,  ..., -0.0207,  0.0446, -0.0540],\n",
       "                      [ 0.0645,  0.0021,  0.0353,  ..., -0.0866,  0.0705, -0.0607],\n",
       "                      [ 0.3234, -0.0855,  0.0420,  ..., -0.0132,  0.1451, -0.0341],\n",
       "                      ...,\n",
       "                      [-0.0828, -0.0605, -0.0142,  ...,  0.0303, -0.0372,  0.0408],\n",
       "                      [ 0.0032,  0.0090, -0.0580,  ..., -0.0326,  0.0133,  0.0202],\n",
       "                      [ 0.0098, -0.0193, -0.0492,  ...,  0.0284,  0.0203, -0.0471]])),\n",
       "             ('e.e_layers.1.ffns.linear2.bias',\n",
       "              tensor([-0.1693,  0.0458,  0.1152,  ...,  0.0152, -0.1349, -0.0050])),\n",
       "             ('e.e_layers.1.layer_norm1.weight',\n",
       "              tensor([0.4464, 0.4573, 0.2052,  ..., 0.4819, 0.5157, 0.5289])),\n",
       "             ('e.e_layers.1.layer_norm1.bias',\n",
       "              tensor([ 0.0711,  0.1116, -0.0349,  ...,  0.0249,  0.0126,  0.0822])),\n",
       "             ('e.e_layers.1.layer_norm2.weight',\n",
       "              tensor([0.3813, 0.3549, 0.2129,  ..., 0.3906, 0.3863, 0.3988])),\n",
       "             ('e.e_layers.1.layer_norm2.bias',\n",
       "              tensor([-0.0040, -0.0105,  0.0248,  ...,  0.0127,  0.0244,  0.0067])),\n",
       "             ('e.e_layers.2.attention.linearQ.weight',\n",
       "              tensor([[ 0.0944,  0.1376, -0.0052,  ...,  0.0017, -0.0706,  0.0862],\n",
       "                      [ 0.0963,  0.1002,  0.0458,  ...,  0.0072,  0.1141,  0.0077],\n",
       "                      [ 0.0137,  0.0512,  0.0273,  ..., -0.0159,  0.0261, -0.0265],\n",
       "                      ...,\n",
       "                      [ 0.0764,  0.1174, -0.0705,  ...,  0.1199, -0.0780, -0.0114],\n",
       "                      [ 0.0631,  0.0276, -0.1013,  ..., -0.0099,  0.0174, -0.0234],\n",
       "                      [ 0.0520, -0.0293, -0.0222,  ...,  0.0536, -0.0128, -0.0131]])),\n",
       "             ('e.e_layers.2.attention.linearQ.bias',\n",
       "              tensor([ 0.0941,  0.6780, -0.1700,  ..., -0.2051, -0.0418, -0.1207])),\n",
       "             ('e.e_layers.2.attention.linearK.weight',\n",
       "              tensor([[ 0.0481, -0.0655,  0.0602,  ..., -0.0486, -0.0086,  0.1311],\n",
       "                      [-0.0849,  0.0454,  0.0667,  ..., -0.0463, -0.1376, -0.0595],\n",
       "                      [ 0.0097, -0.0135,  0.0243,  ...,  0.1451,  0.1266,  0.0509],\n",
       "                      ...,\n",
       "                      [-0.0345, -0.0749, -0.0609,  ...,  0.0346,  0.0181, -0.0157],\n",
       "                      [-0.0546,  0.0862,  0.0122,  ..., -0.0271,  0.0644,  0.0386],\n",
       "                      [ 0.0729, -0.0366, -0.0419,  ..., -0.0935, -0.0383,  0.0348]])),\n",
       "             ('e.e_layers.2.attention.linearK.bias',\n",
       "              tensor([-0.0226,  0.0436, -0.0008,  ...,  0.0204,  0.0308, -0.0114])),\n",
       "             ('e.e_layers.2.attention.linearV.weight',\n",
       "              tensor([[ 0.0069,  0.0065, -0.0128,  ..., -0.0207, -0.0346, -0.0324],\n",
       "                      [ 0.0189,  0.0273, -0.0403,  ..., -0.0363, -0.0111,  0.0200],\n",
       "                      [ 0.0100, -0.0480, -0.0731,  ..., -0.0158, -0.0028, -0.0145],\n",
       "                      ...,\n",
       "                      [-0.0325, -0.0163,  0.0046,  ..., -0.0150,  0.1001, -0.0689],\n",
       "                      [ 0.0214, -0.0262, -0.0477,  ..., -0.0067, -0.0232,  0.0274],\n",
       "                      [-0.0131, -0.0238, -0.1801,  ..., -0.0302, -0.0241,  0.0693]])),\n",
       "             ('e.e_layers.2.attention.linearV.bias',\n",
       "              tensor([ 0.0729, -0.0317, -0.0741,  ...,  0.0542,  0.0077, -0.0015])),\n",
       "             ('e.e_layers.2.attention.out.weight',\n",
       "              tensor([[ 0.0148, -0.0281, -0.0371,  ...,  0.0416,  0.0285, -0.0226],\n",
       "                      [ 0.0277,  0.0121, -0.0389,  ...,  0.0235,  0.0316,  0.0248],\n",
       "                      [-0.0370, -0.0513,  0.0026,  ..., -0.0375,  0.0174,  0.0056],\n",
       "                      ...,\n",
       "                      [ 0.0467, -0.0335,  0.0186,  ...,  0.0317, -0.0318,  0.0282],\n",
       "                      [ 0.0388,  0.0125, -0.0315,  ..., -0.0421, -0.0224, -0.0082],\n",
       "                      [ 0.0416,  0.0026,  0.0131,  ...,  0.0494,  0.0533, -0.0521]])),\n",
       "             ('e.e_layers.2.attention.out.bias',\n",
       "              tensor([-0.0294, -0.0241, -0.0048,  ...,  0.0231,  0.0118, -0.0042])),\n",
       "             ('e.e_layers.2.ffns.linear1.weight',\n",
       "              tensor([[-0.0555, -0.0608, -0.1138,  ..., -0.0670,  0.0080,  0.0545],\n",
       "                      [-0.0851, -0.0643,  0.1356,  ..., -0.0211,  0.0099, -0.0086],\n",
       "                      [-0.0731, -0.0885, -0.0511,  ..., -0.0264,  0.0881,  0.0139],\n",
       "                      ...,\n",
       "                      [-0.0278,  0.0843, -0.0577,  ..., -0.0185,  0.0492, -0.0237],\n",
       "                      [ 0.0757, -0.0294,  0.0648,  ...,  0.0603,  0.0411,  0.0124],\n",
       "                      [-0.0005,  0.0399,  0.0358,  ..., -0.0807, -0.0368,  0.0218]])),\n",
       "             ('e.e_layers.2.ffns.linear1.bias',\n",
       "              tensor([-0.2317, -0.0297, -0.0170,  ..., -0.0269, -0.0045, -0.1322])),\n",
       "             ('e.e_layers.2.ffns.linear2.weight',\n",
       "              tensor([[-0.0179, -0.0154, -0.0121,  ..., -0.0265, -0.0185, -0.0230],\n",
       "                      [ 0.0102,  0.0245,  0.0696,  ..., -0.0655,  0.0690, -0.0257],\n",
       "                      [ 0.0441, -0.0594, -0.0022,  ...,  0.0462,  0.0473,  0.0727],\n",
       "                      ...,\n",
       "                      [-0.0716,  0.0544,  0.0204,  ...,  0.0453,  0.0053,  0.0285],\n",
       "                      [-0.0250, -0.0231,  0.0367,  ...,  0.0355, -0.0394, -0.0462],\n",
       "                      [-0.0030, -0.0017, -0.0376,  ..., -0.0309,  0.0203,  0.0081]])),\n",
       "             ('e.e_layers.2.ffns.linear2.bias',\n",
       "              tensor([-0.0820,  0.0978, -0.0692,  ...,  0.1123, -0.1595,  0.0350])),\n",
       "             ('e.e_layers.2.layer_norm1.weight',\n",
       "              tensor([0.4768, 0.4742, 0.2371,  ..., 0.5040, 0.5466, 0.5715])),\n",
       "             ('e.e_layers.2.layer_norm1.bias',\n",
       "              tensor([ 0.0302,  0.0957,  0.0438,  ...,  0.0707, -0.0195,  0.0667])),\n",
       "             ('e.e_layers.2.layer_norm2.weight',\n",
       "              tensor([0.3681, 0.3524, 0.2329,  ..., 0.3809, 0.3719, 0.3847])),\n",
       "             ('e.e_layers.2.layer_norm2.bias',\n",
       "              tensor([ 0.0094, -0.0063,  0.0116,  ...,  0.0081,  0.0325,  0.0089])),\n",
       "             ('e.e_layers.3.attention.linearQ.weight',\n",
       "              tensor([[-0.0986,  0.0676,  0.0609,  ..., -0.0022,  0.1726,  0.0367],\n",
       "                      [ 0.1591,  0.0678,  0.0429,  ...,  0.0129, -0.0004,  0.0308],\n",
       "                      [ 0.0271,  0.0512, -0.0735,  ..., -0.0903, -0.0229,  0.0319],\n",
       "                      ...,\n",
       "                      [ 0.0023,  0.0193,  0.0217,  ..., -0.0349, -0.0140, -0.0679],\n",
       "                      [-0.0215, -0.1003,  0.0572,  ...,  0.0413, -0.2454,  0.0465],\n",
       "                      [-0.0418, -0.0130, -0.1526,  ...,  0.1621,  0.0046, -0.0616]])),\n",
       "             ('e.e_layers.3.attention.linearQ.bias',\n",
       "              tensor([-0.0961,  0.0035, -0.0896,  ...,  0.0112, -0.3032,  0.1218])),\n",
       "             ('e.e_layers.3.attention.linearK.weight',\n",
       "              tensor([[ 0.0228, -0.0457,  0.0192,  ..., -0.0496, -0.1007,  0.0120],\n",
       "                      [-0.2532, -0.0131,  0.1818,  ..., -0.0654,  0.0049, -0.0045],\n",
       "                      [-0.0100, -0.1318,  0.1282,  ...,  0.0726,  0.0973,  0.0507],\n",
       "                      ...,\n",
       "                      [ 0.0160, -0.1095, -0.0528,  ...,  0.0390,  0.0122, -0.0233],\n",
       "                      [ 0.0030,  0.0899, -0.0269,  ..., -0.0169, -0.1134, -0.0585],\n",
       "                      [ 0.1122, -0.0940,  0.0716,  ..., -0.0474, -0.0773,  0.1736]])),\n",
       "             ('e.e_layers.3.attention.linearK.bias',\n",
       "              tensor([-0.0183,  0.0041, -0.0290,  ...,  0.0175,  0.0861,  0.0065])),\n",
       "             ('e.e_layers.3.attention.linearV.weight',\n",
       "              tensor([[ 0.0100,  0.0226,  0.0209,  ..., -0.0076,  0.0332,  0.0712],\n",
       "                      [ 0.0202,  0.0029,  0.0223,  ...,  0.0139,  0.0496, -0.0055],\n",
       "                      [ 0.0816, -0.0211, -0.0502,  ..., -0.0487, -0.0500, -0.0600],\n",
       "                      ...,\n",
       "                      [ 0.0336,  0.0081,  0.0722,  ..., -0.1011, -0.0272, -0.0117],\n",
       "                      [-0.0426, -0.0431,  0.0081,  ..., -0.0267, -0.0214,  0.0029],\n",
       "                      [ 0.0278, -0.0229,  0.0423,  ..., -0.0403,  0.0177,  0.0294]])),\n",
       "             ('e.e_layers.3.attention.linearV.bias',\n",
       "              tensor([-0.0352, -0.0183,  0.0004,  ..., -0.0107,  0.0209, -0.0248])),\n",
       "             ('e.e_layers.3.attention.out.weight',\n",
       "              tensor([[ 0.0489,  0.0332,  0.0306,  ...,  0.0202,  0.0208, -0.0470],\n",
       "                      [ 0.0048, -0.0277, -0.0529,  ..., -0.0181,  0.0193,  0.0036],\n",
       "                      [-0.0188, -0.0113,  0.0065,  ..., -0.0482,  0.0512, -0.0237],\n",
       "                      ...,\n",
       "                      [-0.0407,  0.0265, -0.0062,  ..., -0.0127,  0.0127,  0.0217],\n",
       "                      [ 0.0257, -0.0454, -0.0460,  ...,  0.0436,  0.0523,  0.0358],\n",
       "                      [ 0.0055, -0.0503, -0.0114,  ...,  0.0261, -0.0282, -0.0269]])),\n",
       "             ('e.e_layers.3.attention.out.bias',\n",
       "              tensor([-0.0294, -0.0241, -0.0048,  ...,  0.0231,  0.0118, -0.0042])),\n",
       "             ('e.e_layers.3.ffns.linear1.weight',\n",
       "              tensor([[-6.9625e-02, -1.6698e-01, -2.2679e-01,  ...,  2.1818e-02,\n",
       "                       -2.9215e-02, -8.5102e-02],\n",
       "                      [ 2.3410e-03, -7.1822e-02, -9.9518e-03,  ..., -6.6059e-03,\n",
       "                        5.9896e-03,  4.3905e-02],\n",
       "                      [-7.0741e-01,  3.5792e-01,  5.0494e-01,  ...,  2.6759e-02,\n",
       "                       -6.4477e-01, -1.9407e-01],\n",
       "                      ...,\n",
       "                      [ 1.3251e-02, -1.4132e-02,  8.8962e-02,  ...,  2.1511e-02,\n",
       "                        2.7720e-02,  2.9540e-02],\n",
       "                      [-3.0268e-02, -3.9998e-04,  6.4605e-02,  ..., -3.8746e-02,\n",
       "                        3.0839e-02,  7.0653e-02],\n",
       "                      [ 2.4849e-02, -3.6046e-02, -2.1855e-02,  ...,  4.3703e-02,\n",
       "                        1.2780e-01, -8.3130e-03]])),\n",
       "             ('e.e_layers.3.ffns.linear1.bias',\n",
       "              tensor([-0.4254, -0.2217, -0.4353,  ...,  0.0726, -0.0057, -0.2265])),\n",
       "             ('e.e_layers.3.ffns.linear2.weight',\n",
       "              tensor([[-0.0087,  0.0155,  0.0261,  ...,  0.0532, -0.0169, -0.0326],\n",
       "                      [-0.0572, -0.0170, -0.0075,  ...,  0.0084,  0.0432, -0.0525],\n",
       "                      [-0.0211, -0.0532,  0.0041,  ...,  0.0414,  0.0471,  0.0112],\n",
       "                      ...,\n",
       "                      [-0.0852,  0.0935,  0.0130,  ..., -0.0170,  0.0343, -0.0529],\n",
       "                      [-0.0656, -0.0346,  0.0122,  ...,  0.0175,  0.0088,  0.0683],\n",
       "                      [-0.0739, -0.0112, -0.0120,  ...,  0.0344, -0.0371, -0.0204]])),\n",
       "             ('e.e_layers.3.ffns.linear2.bias',\n",
       "              tensor([-0.1074,  0.0315,  0.0992,  ...,  0.0824, -0.0823,  0.0817])),\n",
       "             ('e.e_layers.3.layer_norm1.weight',\n",
       "              tensor([0.4364, 0.4422, 0.1895,  ..., 0.4828, 0.5303, 0.5440])),\n",
       "             ('e.e_layers.3.layer_norm1.bias',\n",
       "              tensor([0.0910, 0.0779, 0.0233,  ..., 0.1450, 0.0090, 0.0678])),\n",
       "             ('e.e_layers.3.layer_norm2.weight',\n",
       "              tensor([0.3507, 0.3377, 0.2421,  ..., 0.3512, 0.3481, 0.3586])),\n",
       "             ('e.e_layers.3.layer_norm2.bias',\n",
       "              tensor([-0.0080, -0.0066,  0.0078,  ..., -0.0162,  0.0304,  0.0046])),\n",
       "             ('e.e_layers.4.attention.linearQ.weight',\n",
       "              tensor([[-0.0044,  0.0102,  0.0674,  ..., -0.0358,  0.0065, -0.0406],\n",
       "                      [ 0.1437,  0.0385,  0.0739,  ..., -0.0539,  0.0343,  0.1382],\n",
       "                      [ 0.1419, -0.0896,  0.1168,  ..., -0.0088,  0.0408,  0.0083],\n",
       "                      ...,\n",
       "                      [-0.0470, -0.1338,  0.1144,  ...,  0.1082, -0.0097, -0.1623],\n",
       "                      [-0.1064, -0.0884, -0.0754,  ..., -0.1307, -0.0355, -0.1175],\n",
       "                      [-0.0346,  0.0433,  0.0810,  ...,  0.1477,  0.1068, -0.0816]])),\n",
       "             ('e.e_layers.4.attention.linearQ.bias',\n",
       "              tensor([-0.0208, -0.1714,  0.1830,  ...,  0.1395, -0.1062,  0.3165])),\n",
       "             ('e.e_layers.4.attention.linearK.weight',\n",
       "              tensor([[ 0.1057,  0.0555,  0.0465,  ...,  0.0923,  0.1952, -0.0868],\n",
       "                      [ 0.0043, -0.1363, -0.0451,  ..., -0.0364, -0.0206, -0.0834],\n",
       "                      [-0.1063, -0.0111, -0.0738,  ...,  0.0732,  0.0467,  0.2091],\n",
       "                      ...,\n",
       "                      [-0.0857,  0.0307, -0.0029,  ...,  0.0394,  0.0312, -0.0444],\n",
       "                      [ 0.0094, -0.1128,  0.1751,  ..., -0.1277, -0.1753, -0.0409],\n",
       "                      [ 0.0233, -0.1867,  0.0131,  ..., -0.0528,  0.0665,  0.0181]])),\n",
       "             ('e.e_layers.4.attention.linearK.bias',\n",
       "              tensor([-0.0020,  0.0318,  0.0296,  ...,  0.0141,  0.0087,  0.0330])),\n",
       "             ('e.e_layers.4.attention.linearV.weight',\n",
       "              tensor([[ 2.0351e-03,  4.1648e-02,  1.6443e-01,  ..., -1.5152e-01,\n",
       "                        1.9675e-02,  4.1501e-02],\n",
       "                      [ 1.1681e-01, -3.2705e-02,  1.5265e-02,  ...,  9.6724e-02,\n",
       "                        4.1497e-06,  3.6233e-02],\n",
       "                      [ 4.1868e-03,  1.9621e-02, -1.0358e-01,  ..., -3.2099e-03,\n",
       "                        3.6373e-02,  6.9405e-02],\n",
       "                      ...,\n",
       "                      [ 2.3999e-02,  2.7746e-02,  6.4934e-03,  ...,  9.2423e-03,\n",
       "                        3.4963e-02,  5.3026e-02],\n",
       "                      [ 9.2460e-03, -4.0338e-02, -1.6015e-02,  ...,  2.2040e-02,\n",
       "                        7.5845e-02, -5.3213e-02],\n",
       "                      [-5.7953e-02,  4.4128e-02, -2.8196e-02,  ..., -1.8137e-02,\n",
       "                        7.3454e-02,  2.5076e-02]])),\n",
       "             ('e.e_layers.4.attention.linearV.bias',\n",
       "              tensor([ 0.0525, -0.0300,  0.0137,  ...,  0.0540,  0.0079, -0.0395])),\n",
       "             ('e.e_layers.4.attention.out.weight',\n",
       "              tensor([[-0.0100,  0.0250, -0.0105,  ...,  0.0136, -0.0333, -0.0506],\n",
       "                      [-0.0007,  0.0064, -0.0231,  ...,  0.0467,  0.0018, -0.0009],\n",
       "                      [-0.0452,  0.0018, -0.0525,  ..., -0.0265,  0.0002,  0.0458],\n",
       "                      ...,\n",
       "                      [-0.0364,  0.0437, -0.0396,  ...,  0.0412, -0.0465,  0.0187],\n",
       "                      [ 0.0006,  0.0205,  0.0454,  ..., -0.0391,  0.0465, -0.0065],\n",
       "                      [ 0.0160,  0.0376, -0.0314,  ...,  0.0055,  0.0156, -0.0430]])),\n",
       "             ('e.e_layers.4.attention.out.bias',\n",
       "              tensor([-0.0294, -0.0241, -0.0048,  ...,  0.0231,  0.0118, -0.0042])),\n",
       "             ('e.e_layers.4.ffns.linear1.weight',\n",
       "              tensor([[ 0.0216,  0.0751, -0.0569,  ..., -0.0129, -0.0270, -0.0024],\n",
       "                      [ 0.0344, -0.0152, -0.0054,  ..., -0.0230, -0.0335,  0.0385],\n",
       "                      [-0.0698,  0.0146, -0.0915,  ..., -0.0228,  0.0406, -0.0791],\n",
       "                      ...,\n",
       "                      [-0.0776, -0.0350,  0.0499,  ..., -0.0129, -0.1249, -0.0452],\n",
       "                      [ 0.0522, -0.0472, -0.0136,  ...,  0.0466, -0.0526, -0.0050],\n",
       "                      [ 0.0223,  0.0142,  0.0422,  ..., -0.0072, -0.0450,  0.0151]])),\n",
       "             ('e.e_layers.4.ffns.linear1.bias',\n",
       "              tensor([ 0.0089, -0.2012, -0.1557,  ..., -0.1270,  0.0273, -0.1238])),\n",
       "             ('e.e_layers.4.ffns.linear2.weight',\n",
       "              tensor([[ 0.0040, -0.0721, -0.0324,  ..., -0.0864, -0.0166, -0.0040],\n",
       "                      [-0.0312,  0.0026,  0.0074,  ...,  0.0431,  0.0127,  0.0259],\n",
       "                      [ 0.0513,  0.0423, -0.0184,  ..., -0.0292,  0.1080, -0.0010],\n",
       "                      ...,\n",
       "                      [ 0.0277,  0.0695, -0.0328,  ..., -0.0380,  0.0293,  0.0162],\n",
       "                      [ 0.0191, -0.0428, -0.0040,  ..., -0.0354,  0.0069,  0.0034],\n",
       "                      [ 0.0196, -0.0226, -0.0319,  ..., -0.0722, -0.0328, -0.0495]])),\n",
       "             ('e.e_layers.4.ffns.linear2.bias',\n",
       "              tensor([-0.0305,  0.0043, -0.0613,  ..., -0.0653, -0.3320, -0.0215])),\n",
       "             ('e.e_layers.4.layer_norm1.weight',\n",
       "              tensor([0.4332, 0.4392, 0.2770,  ..., 0.4634, 0.5124, 0.5299])),\n",
       "             ('e.e_layers.4.layer_norm1.bias',\n",
       "              tensor([ 0.0232,  0.1131,  0.0252,  ...,  0.0516, -0.0252,  0.0351])),\n",
       "             ('e.e_layers.4.layer_norm2.weight',\n",
       "              tensor([0.4054, 0.4021, 0.3370,  ..., 0.4128, 0.4009, 0.4142])),\n",
       "             ('e.e_layers.4.layer_norm2.bias',\n",
       "              tensor([ 0.0116, -0.0017,  0.0173,  ...,  0.0156,  0.0524,  0.0232])),\n",
       "             ('e.e_layers.5.attention.linearQ.weight',\n",
       "              tensor([[ 0.0852,  0.1012,  0.0978,  ..., -0.0792, -0.0244, -0.0164],\n",
       "                      [ 0.0170,  0.0793, -0.0351,  ...,  0.1482,  0.0174,  0.0707],\n",
       "                      [ 0.0941,  0.1224, -0.0831,  ...,  0.0243, -0.0296,  0.0639],\n",
       "                      ...,\n",
       "                      [ 0.0254, -0.0403,  0.0097,  ...,  0.1160,  0.0238,  0.0762],\n",
       "                      [-0.0349, -0.0175, -0.0616,  ...,  0.1488,  0.1092,  0.0140],\n",
       "                      [-0.0502, -0.0385,  0.0042,  ..., -0.0733,  0.0389, -0.0358]])),\n",
       "             ('e.e_layers.5.attention.linearQ.bias',\n",
       "              tensor([ 0.1946,  0.1501,  0.1053,  ...,  0.3054, -0.0996,  0.0542])),\n",
       "             ('e.e_layers.5.attention.linearK.weight',\n",
       "              tensor([[ 0.0780, -0.0202,  0.0056,  ...,  0.0462,  0.0197, -0.1847],\n",
       "                      [ 0.0495,  0.0496,  0.0216,  ...,  0.0495,  0.0752,  0.1669],\n",
       "                      [-0.0696,  0.0283,  0.0652,  ...,  0.0274, -0.1012, -0.1302],\n",
       "                      ...,\n",
       "                      [ 0.0532, -0.0071, -0.0907,  ...,  0.0028, -0.2750, -0.1537],\n",
       "                      [ 0.1811,  0.0298, -0.0234,  ...,  0.0566,  0.1626, -0.1257],\n",
       "                      [ 0.0071, -0.0978,  0.1203,  ..., -0.0103,  0.0959,  0.0261]])),\n",
       "             ('e.e_layers.5.attention.linearK.bias',\n",
       "              tensor([ 0.0477,  0.0353,  0.0635,  ..., -0.0364,  0.0408,  0.0443])),\n",
       "             ('e.e_layers.5.attention.linearV.weight',\n",
       "              tensor([[-0.0494, -0.0009, -0.0096,  ..., -0.0449,  0.0041, -0.0074],\n",
       "                      [ 0.0673, -0.0359, -0.0293,  ..., -0.0460, -0.0272, -0.1125],\n",
       "                      [ 0.0527, -0.0347, -0.0091,  ...,  0.0191,  0.0048,  0.0006],\n",
       "                      ...,\n",
       "                      [ 0.0049,  0.0336,  0.0153,  ...,  0.0911, -0.0442,  0.0479],\n",
       "                      [ 0.0337,  0.0355,  0.0384,  ..., -0.0281,  0.0313, -0.0204],\n",
       "                      [ 0.0575, -0.0180, -0.0780,  ...,  0.0264, -0.0920,  0.0188]])),\n",
       "             ('e.e_layers.5.attention.linearV.bias',\n",
       "              tensor([-0.0462, -0.0395,  0.0189,  ...,  0.0251,  0.0747, -0.0431])),\n",
       "             ('e.e_layers.5.attention.out.weight',\n",
       "              tensor([[ 0.0052,  0.0486,  0.0255,  ..., -0.0226,  0.0218, -0.0207],\n",
       "                      [ 0.0532, -0.0272, -0.0119,  ...,  0.0254, -0.0437,  0.0163],\n",
       "                      [-0.0311,  0.0318, -0.0186,  ...,  0.0236,  0.0230,  0.0521],\n",
       "                      ...,\n",
       "                      [ 0.0127,  0.0188, -0.0510,  ..., -0.0141,  0.0384,  0.0222],\n",
       "                      [ 0.0533, -0.0539,  0.0129,  ..., -0.0364, -0.0272, -0.0283],\n",
       "                      [-0.0176, -0.0094, -0.0404,  ..., -0.0194,  0.0039, -0.0076]])),\n",
       "             ('e.e_layers.5.attention.out.bias',\n",
       "              tensor([-0.0294, -0.0241, -0.0048,  ...,  0.0231,  0.0118, -0.0042])),\n",
       "             ('e.e_layers.5.ffns.linear1.weight',\n",
       "              tensor([[-0.0100, -0.0504,  0.0171,  ...,  0.0218, -0.0028,  0.0202],\n",
       "                      [ 0.0391, -0.0372,  0.0385,  ..., -0.0062,  0.0111, -0.0325],\n",
       "                      [-0.0122, -0.0170, -0.0189,  ...,  0.0126,  0.0046, -0.0322],\n",
       "                      ...,\n",
       "                      [ 0.0286,  0.0261,  0.0013,  ..., -0.0159, -0.0136, -0.0088],\n",
       "                      [ 0.0498, -0.0212, -0.0348,  ...,  0.0263, -0.0318, -0.0394],\n",
       "                      [ 0.0171, -0.0505, -0.0144,  ...,  0.0146,  0.0050,  0.0124]])),\n",
       "             ('e.e_layers.5.ffns.linear1.bias',\n",
       "              tensor([-0.1056, -0.1680, -0.0700,  ..., -0.0004, -0.0529, -0.1539])),\n",
       "             ('e.e_layers.5.ffns.linear2.weight',\n",
       "              tensor([[ 0.0051,  0.0295,  0.0540,  ...,  0.0090,  0.0325,  0.0044],\n",
       "                      [ 0.0209, -0.0039, -0.0026,  ..., -0.0451,  0.0199,  0.0157],\n",
       "                      [ 0.0493, -0.0232, -0.0652,  ..., -0.0930, -0.0213,  0.0274],\n",
       "                      ...,\n",
       "                      [ 0.0203,  0.0559, -0.0607,  ..., -0.0402, -0.0022, -0.0680],\n",
       "                      [-0.0132, -0.0096, -0.0482,  ..., -0.0335, -0.0140,  0.0189],\n",
       "                      [-0.0652, -0.0032,  0.0133,  ..., -0.0196, -0.0393, -0.0709]])),\n",
       "             ('e.e_layers.5.ffns.linear2.bias',\n",
       "              tensor([ 0.0816,  0.0328, -0.0995,  ..., -0.0380, -0.1476, -0.0716])),\n",
       "             ('e.e_layers.5.layer_norm1.weight',\n",
       "              tensor([0.4685, 0.4853, 0.3902,  ..., 0.4730, 0.5013, 0.5023])),\n",
       "             ('e.e_layers.5.layer_norm1.bias',\n",
       "              tensor([ 0.0401,  0.0435,  0.0114,  ...,  0.0150, -0.0327,  0.0138])),\n",
       "             ('e.e_layers.5.layer_norm2.weight',\n",
       "              tensor([1.1205, 1.0961, 0.8306,  ..., 1.0240, 1.0755, 1.0793])),\n",
       "             ('e.e_layers.5.layer_norm2.bias',\n",
       "              tensor([ 0.0387, -0.0461, -0.1217,  ..., -0.0325, -0.0772, -0.0336])),\n",
       "             ('d.embedder.embed.weight',\n",
       "              tensor([[-0.0013, -0.0010,  0.0053,  ...,  0.0017, -0.0076,  0.0094],\n",
       "                      [ 0.0081, -0.0052, -0.0047,  ..., -0.0084,  0.0071, -0.0040],\n",
       "                      [ 0.0033, -0.0018, -0.0039,  ..., -0.0005,  0.0065,  0.0049],\n",
       "                      ...,\n",
       "                      [ 0.0094,  0.0089,  0.0091,  ...,  0.0029,  0.0091,  0.0017],\n",
       "                      [-0.0037,  0.0094,  0.0092,  ..., -0.0045, -0.0019, -0.0037],\n",
       "                      [ 0.0081,  0.0012,  0.0037,  ..., -0.0046,  0.0018,  0.0002]])),\n",
       "             ('d.pe.pe',\n",
       "              tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
       "                        0.0000e+00,  1.0000e+00],\n",
       "                      [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
       "                        1.0366e-08,  1.0000e+00],\n",
       "                      [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
       "                        2.0733e-08,  1.0000e+00],\n",
       "                      ...,\n",
       "                      [ 6.1950e-02,  9.9808e-01,  7.9821e-01,  ...,  1.0000e+00,\n",
       "                        5.2765e-06,  1.0000e+00],\n",
       "                      [ 8.7333e-01,  4.8714e-01,  9.4981e-01,  ...,  1.0000e+00,\n",
       "                        5.2868e-06,  1.0000e+00],\n",
       "                      [ 8.8177e-01, -4.7168e-01,  2.8400e-01,  ...,  1.0000e+00,\n",
       "                        5.2972e-06,  1.0000e+00]])),\n",
       "             ('d.d_layers.0.attention.linearQ.weight',\n",
       "              tensor([[-0.0087, -0.1793,  0.0245,  ...,  0.1347, -0.0830, -0.1262],\n",
       "                      [-0.0476, -0.0668,  0.1115,  ...,  0.0627,  0.0377, -0.0942],\n",
       "                      [ 0.0377,  0.1061,  0.0560,  ..., -0.0478, -0.0238, -0.0410],\n",
       "                      ...,\n",
       "                      [-0.0754, -0.0742,  0.2679,  ...,  0.0081, -0.0088,  0.0206],\n",
       "                      [-0.0698, -0.0037, -0.0277,  ...,  0.0625,  0.0602, -0.0348],\n",
       "                      [-0.1392, -0.0295, -0.0606,  ..., -0.0315,  0.0156, -0.0347]])),\n",
       "             ('d.d_layers.0.attention.linearQ.bias',\n",
       "              tensor([0.0375, 0.3299, 0.0121,  ..., 0.0533, 0.1462, 0.1385])),\n",
       "             ('d.d_layers.0.attention.linearK.weight',\n",
       "              tensor([[ 0.0702,  0.0082, -0.0908,  ..., -0.0864,  0.0325,  0.0618],\n",
       "                      [ 0.0567, -0.0846,  0.1303,  ...,  0.0646,  0.0282,  0.0964],\n",
       "                      [ 0.0674, -0.0822, -0.0075,  ..., -0.0487,  0.0630, -0.1170],\n",
       "                      ...,\n",
       "                      [-0.0398, -0.0953,  0.0399,  ...,  0.1075,  0.0879, -0.0253],\n",
       "                      [ 0.0473,  0.0310, -0.2221,  ...,  0.0166, -0.0768,  0.0466],\n",
       "                      [ 0.0505,  0.0816,  0.1128,  ...,  0.0025, -0.0634,  0.0541]])),\n",
       "             ('d.d_layers.0.attention.linearK.bias',\n",
       "              tensor([-0.0068, -0.0168, -0.0112,  ..., -0.0190, -0.0251, -0.0015])),\n",
       "             ('d.d_layers.0.attention.linearV.weight',\n",
       "              tensor([[ 0.0040, -0.0074, -0.0066,  ..., -0.0269,  0.0299, -0.0202],\n",
       "                      [ 0.0477, -0.0207,  0.0066,  ..., -0.0238,  0.0273,  0.0186],\n",
       "                      [-0.0112,  0.0097,  0.0115,  ...,  0.0251,  0.0149,  0.0284],\n",
       "                      ...,\n",
       "                      [ 0.0343,  0.0285,  0.0078,  ...,  0.0451,  0.0414, -0.0044],\n",
       "                      [-0.0761, -0.0317,  0.0331,  ..., -0.0622, -0.0022,  0.0261],\n",
       "                      [-0.0062,  0.0272, -0.0020,  ...,  0.0136, -0.0079,  0.0159]])),\n",
       "             ('d.d_layers.0.attention.linearV.bias',\n",
       "              tensor([-0.0045, -0.0045, -0.0061,  ...,  0.0197,  0.0057,  0.0140])),\n",
       "             ('d.d_layers.0.attention.out.weight',\n",
       "              tensor([[-0.0147,  0.0061,  0.0120,  ...,  0.0464, -0.0071, -0.0108],\n",
       "                      [ 0.0233, -0.0538, -0.0408,  ..., -0.0430, -0.0540,  0.0396],\n",
       "                      [-0.0335,  0.0192, -0.0092,  ..., -0.0453,  0.0471, -0.0230],\n",
       "                      ...,\n",
       "                      [ 0.0195,  0.0028, -0.0364,  ...,  0.0084,  0.0357,  0.0092],\n",
       "                      [ 0.0275, -0.0069, -0.0250,  ...,  0.0479, -0.0142, -0.0297],\n",
       "                      [-0.0394, -0.0095, -0.0522,  ..., -0.0518, -0.0419,  0.0305]])),\n",
       "             ('d.d_layers.0.attention.out.bias',\n",
       "              tensor([-0.0004, -0.0107, -0.0041,  ..., -0.0241, -0.0124, -0.0184])),\n",
       "             ('d.d_layers.0.ffns.linear1.weight',\n",
       "              tensor([[-0.0245,  0.0142,  0.0445,  ..., -0.0016,  0.0058,  0.0004],\n",
       "                      [ 0.0236,  0.0234, -0.0023,  ...,  0.0327,  0.0019,  0.0490],\n",
       "                      [-0.0042,  0.0848,  0.0695,  ...,  0.0037,  0.0140, -0.0526],\n",
       "                      ...,\n",
       "                      [ 0.0148,  0.0019,  0.0427,  ...,  0.0258, -0.0592,  0.0224],\n",
       "                      [-0.0926,  0.0477,  0.0153,  ..., -0.0148,  0.0290, -0.0060],\n",
       "                      [ 0.0328, -0.0752, -0.0071,  ...,  0.0031,  0.0016, -0.0341]])),\n",
       "             ('d.d_layers.0.ffns.linear1.bias',\n",
       "              tensor([ 0.0376, -0.0361, -0.1560,  ..., -0.1348, -0.0525, -0.3136])),\n",
       "             ('d.d_layers.0.ffns.linear2.weight',\n",
       "              tensor([[-0.0141, -0.0004, -0.0115,  ..., -0.0209, -0.0351,  0.0153],\n",
       "                      [ 0.0403, -0.0246,  0.0095,  ...,  0.0053,  0.1029,  0.0395],\n",
       "                      [-0.0038, -0.0410,  0.0621,  ...,  0.0053, -0.0043, -0.1205],\n",
       "                      ...,\n",
       "                      [-0.0196,  0.0050, -0.0358,  ..., -0.0532, -0.0050,  0.0517],\n",
       "                      [ 0.0351,  0.0351,  0.0264,  ...,  0.0173, -0.0307, -0.0733],\n",
       "                      [-0.0141, -0.0012,  0.0444,  ...,  0.0169,  0.0221, -0.0776]])),\n",
       "             ('d.d_layers.0.ffns.linear2.bias',\n",
       "              tensor([-0.0982,  0.1101, -0.0966,  ..., -0.0024, -0.0465,  0.0259])),\n",
       "             ('d.d_layers.0.layer_norm1.weight',\n",
       "              tensor([0.4805, 0.4905, 0.3896,  ..., 0.5049, 0.5101, 0.5389])),\n",
       "             ('d.d_layers.0.layer_norm1.bias',\n",
       "              tensor([ 0.2052,  0.0375, -0.2659,  ...,  0.0634, -0.0294,  0.0375])),\n",
       "             ('d.d_layers.0.layer_norm2.weight',\n",
       "              tensor([0.3501, 0.3243, 0.2274,  ..., 0.3455, 0.3446, 0.3474])),\n",
       "             ('d.d_layers.0.layer_norm2.bias',\n",
       "              tensor([-0.0402, -0.0095,  0.0860,  ..., -0.0054,  0.0376, -0.0076])),\n",
       "             ('d.d_layers.1.attention.linearQ.weight',\n",
       "              tensor([[-0.0194, -0.0435,  0.0669,  ..., -0.2365,  0.0262, -0.0365],\n",
       "                      [ 0.1898, -0.0872,  0.1022,  ...,  0.0029,  0.1506, -0.0259],\n",
       "                      [-0.0008, -0.1185, -0.0657,  ..., -0.1489, -0.1620,  0.0383],\n",
       "                      ...,\n",
       "                      [ 0.1145,  0.0366,  0.0889,  ..., -0.0849, -0.0265,  0.1727],\n",
       "                      [-0.1635,  0.0250,  0.0674,  ...,  0.0459,  0.2495,  0.0445],\n",
       "                      [ 0.0664,  0.0258,  0.0069,  ..., -0.1510,  0.0592, -0.1491]])),\n",
       "             ('d.d_layers.1.attention.linearQ.bias',\n",
       "              tensor([-0.1197,  0.1806, -0.1089,  ...,  0.2793, -0.1692, -0.1596])),\n",
       "             ('d.d_layers.1.attention.linearK.weight',\n",
       "              tensor([[ 0.0730, -0.0017, -0.1235,  ..., -0.0338, -0.1131, -0.1052],\n",
       "                      [ 0.0701,  0.0164,  0.0638,  ..., -0.0539,  0.0608, -0.0020],\n",
       "                      [ 0.0193,  0.0365, -0.0459,  ...,  0.0681, -0.0525,  0.0184],\n",
       "                      ...,\n",
       "                      [ 0.0022, -0.0891, -0.0529,  ...,  0.0223, -0.0227,  0.2144],\n",
       "                      [-0.0312,  0.1790, -0.1772,  ..., -0.0517, -0.0175,  0.0993],\n",
       "                      [ 0.0397, -0.0193,  0.0153,  ..., -0.1179, -0.0214, -0.2960]])),\n",
       "             ('d.d_layers.1.attention.linearK.bias',\n",
       "              tensor([ 0.0085, -0.0012, -0.0252,  ...,  0.0337, -0.0167, -0.0273])),\n",
       "             ('d.d_layers.1.attention.linearV.weight',\n",
       "              tensor([[-0.0014,  0.0468, -0.0493,  ...,  0.0178, -0.0386,  0.0385],\n",
       "                      [-0.0134,  0.0323, -0.0327,  ...,  0.0424, -0.0623,  0.0242],\n",
       "                      [-0.0358, -0.0341,  0.0175,  ..., -0.0619,  0.0387, -0.0015],\n",
       "                      ...,\n",
       "                      [ 0.0585,  0.0038, -0.0110,  ..., -0.0278,  0.0403,  0.0297],\n",
       "                      [ 0.0382, -0.0092, -0.0093,  ..., -0.0412,  0.0033,  0.0266],\n",
       "                      [ 0.0246, -0.0461,  0.0165,  ..., -0.0615, -0.0186,  0.0643]])),\n",
       "             ('d.d_layers.1.attention.linearV.bias',\n",
       "              tensor([-0.0351,  0.0107, -0.0243,  ...,  0.0291,  0.0540, -0.0563])),\n",
       "             ('d.d_layers.1.attention.out.weight',\n",
       "              tensor([[ 0.0030,  0.0070, -0.0530,  ..., -0.0366,  0.0429,  0.0382],\n",
       "                      [-0.0167, -0.0132, -0.0241,  ...,  0.0202,  0.0209,  0.0281],\n",
       "                      [-0.0089,  0.0204, -0.0189,  ..., -0.0064, -0.0484, -0.0365],\n",
       "                      ...,\n",
       "                      [ 0.0018,  0.0287,  0.0056,  ..., -0.0024,  0.0423,  0.0050],\n",
       "                      [-0.0052, -0.0004, -0.0075,  ...,  0.0138, -0.0291, -0.0023],\n",
       "                      [-0.0493, -0.0137, -0.0507,  ...,  0.0370, -0.0415, -0.0320]])),\n",
       "             ('d.d_layers.1.attention.out.bias',\n",
       "              tensor([-0.0004, -0.0107, -0.0041,  ..., -0.0241, -0.0124, -0.0184])),\n",
       "             ('d.d_layers.1.ffns.linear1.weight',\n",
       "              tensor([[ 0.0106, -0.0934,  0.0356,  ...,  0.0176,  0.0992, -0.0342],\n",
       "                      [-0.0915,  0.0141,  0.0278,  ..., -0.0225,  0.0362, -0.0019],\n",
       "                      [ 0.0457, -0.1302,  0.1214,  ..., -0.0123,  0.0467,  0.0215],\n",
       "                      ...,\n",
       "                      [ 0.0847,  0.0345, -0.0131,  ..., -0.0674, -0.0047, -0.0068],\n",
       "                      [-0.0370, -0.0043, -0.0413,  ..., -0.0587, -0.0135, -0.0305],\n",
       "                      [-0.1740, -0.0796, -0.0050,  ...,  0.0170,  0.0231, -0.0087]])),\n",
       "             ('d.d_layers.1.ffns.linear1.bias',\n",
       "              tensor([-0.0918, -0.0677, -0.0687,  ...,  0.0231, -0.3387, -0.1482])),\n",
       "             ('d.d_layers.1.ffns.linear2.weight',\n",
       "              tensor([[-0.0088,  0.0246, -0.0893,  ..., -0.0207,  0.0446, -0.0540],\n",
       "                      [ 0.0645,  0.0021,  0.0353,  ..., -0.0866,  0.0705, -0.0607],\n",
       "                      [ 0.3234, -0.0855,  0.0420,  ..., -0.0132,  0.1451, -0.0341],\n",
       "                      ...,\n",
       "                      [-0.0828, -0.0605, -0.0142,  ...,  0.0303, -0.0372,  0.0408],\n",
       "                      [ 0.0032,  0.0090, -0.0580,  ..., -0.0326,  0.0133,  0.0202],\n",
       "                      [ 0.0098, -0.0193, -0.0492,  ...,  0.0284,  0.0203, -0.0471]])),\n",
       "             ('d.d_layers.1.ffns.linear2.bias',\n",
       "              tensor([-0.1693,  0.0458,  0.1152,  ...,  0.0152, -0.1349, -0.0050])),\n",
       "             ('d.d_layers.1.layer_norm1.weight',\n",
       "              tensor([0.4464, 0.4573, 0.2052,  ..., 0.4819, 0.5157, 0.5289])),\n",
       "             ('d.d_layers.1.layer_norm1.bias',\n",
       "              tensor([ 0.0711,  0.1116, -0.0349,  ...,  0.0249,  0.0126,  0.0822])),\n",
       "             ('d.d_layers.1.layer_norm2.weight',\n",
       "              tensor([0.3813, 0.3549, 0.2129,  ..., 0.3906, 0.3863, 0.3988])),\n",
       "             ('d.d_layers.1.layer_norm2.bias',\n",
       "              tensor([-0.0040, -0.0105,  0.0248,  ...,  0.0127,  0.0244,  0.0067])),\n",
       "             ('d.d_layers.2.attention.linearQ.weight',\n",
       "              tensor([[ 0.0944,  0.1376, -0.0052,  ...,  0.0017, -0.0706,  0.0862],\n",
       "                      [ 0.0963,  0.1002,  0.0458,  ...,  0.0072,  0.1141,  0.0077],\n",
       "                      [ 0.0137,  0.0512,  0.0273,  ..., -0.0159,  0.0261, -0.0265],\n",
       "                      ...,\n",
       "                      [ 0.0764,  0.1174, -0.0705,  ...,  0.1199, -0.0780, -0.0114],\n",
       "                      [ 0.0631,  0.0276, -0.1013,  ..., -0.0099,  0.0174, -0.0234],\n",
       "                      [ 0.0520, -0.0293, -0.0222,  ...,  0.0536, -0.0128, -0.0131]])),\n",
       "             ('d.d_layers.2.attention.linearQ.bias',\n",
       "              tensor([ 0.0941,  0.6780, -0.1700,  ..., -0.2051, -0.0418, -0.1207])),\n",
       "             ('d.d_layers.2.attention.linearK.weight',\n",
       "              tensor([[ 0.0481, -0.0655,  0.0602,  ..., -0.0486, -0.0086,  0.1311],\n",
       "                      [-0.0849,  0.0454,  0.0667,  ..., -0.0463, -0.1376, -0.0595],\n",
       "                      [ 0.0097, -0.0135,  0.0243,  ...,  0.1451,  0.1266,  0.0509],\n",
       "                      ...,\n",
       "                      [-0.0345, -0.0749, -0.0609,  ...,  0.0346,  0.0181, -0.0157],\n",
       "                      [-0.0546,  0.0862,  0.0122,  ..., -0.0271,  0.0644,  0.0386],\n",
       "                      [ 0.0729, -0.0366, -0.0419,  ..., -0.0935, -0.0383,  0.0348]])),\n",
       "             ('d.d_layers.2.attention.linearK.bias',\n",
       "              tensor([-0.0226,  0.0436, -0.0008,  ...,  0.0204,  0.0308, -0.0114])),\n",
       "             ('d.d_layers.2.attention.linearV.weight',\n",
       "              tensor([[ 0.0069,  0.0065, -0.0128,  ..., -0.0207, -0.0346, -0.0324],\n",
       "                      [ 0.0189,  0.0273, -0.0403,  ..., -0.0363, -0.0111,  0.0200],\n",
       "                      [ 0.0100, -0.0480, -0.0731,  ..., -0.0158, -0.0028, -0.0145],\n",
       "                      ...,\n",
       "                      [-0.0325, -0.0163,  0.0046,  ..., -0.0150,  0.1001, -0.0689],\n",
       "                      [ 0.0214, -0.0262, -0.0477,  ..., -0.0067, -0.0232,  0.0274],\n",
       "                      [-0.0131, -0.0238, -0.1801,  ..., -0.0302, -0.0241,  0.0693]])),\n",
       "             ('d.d_layers.2.attention.linearV.bias',\n",
       "              tensor([ 0.0729, -0.0317, -0.0741,  ...,  0.0542,  0.0077, -0.0015])),\n",
       "             ('d.d_layers.2.attention.out.weight',\n",
       "              tensor([[ 0.0515, -0.0432,  0.0007,  ...,  0.0151, -0.0524,  0.0258],\n",
       "                      [-0.0339, -0.0341, -0.0416,  ..., -0.0048,  0.0070,  0.0525],\n",
       "                      [-0.0430,  0.0018,  0.0210,  ..., -0.0068, -0.0408,  0.0228],\n",
       "                      ...,\n",
       "                      [-0.0464,  0.0206,  0.0431,  ..., -0.0359, -0.0452, -0.0337],\n",
       "                      [-0.0301, -0.0378,  0.0523,  ...,  0.0419,  0.0527,  0.0518],\n",
       "                      [ 0.0028,  0.0167, -0.0049,  ...,  0.0263,  0.0022, -0.0430]])),\n",
       "             ('d.d_layers.2.attention.out.bias',\n",
       "              tensor([-0.0004, -0.0107, -0.0041,  ..., -0.0241, -0.0124, -0.0184])),\n",
       "             ('d.d_layers.2.ffns.linear1.weight',\n",
       "              tensor([[-0.0555, -0.0608, -0.1138,  ..., -0.0670,  0.0080,  0.0545],\n",
       "                      [-0.0851, -0.0643,  0.1356,  ..., -0.0211,  0.0099, -0.0086],\n",
       "                      [-0.0731, -0.0885, -0.0511,  ..., -0.0264,  0.0881,  0.0139],\n",
       "                      ...,\n",
       "                      [-0.0278,  0.0843, -0.0577,  ..., -0.0185,  0.0492, -0.0237],\n",
       "                      [ 0.0757, -0.0294,  0.0648,  ...,  0.0603,  0.0411,  0.0124],\n",
       "                      [-0.0005,  0.0399,  0.0358,  ..., -0.0807, -0.0368,  0.0218]])),\n",
       "             ('d.d_layers.2.ffns.linear1.bias',\n",
       "              tensor([-0.2317, -0.0297, -0.0170,  ..., -0.0269, -0.0045, -0.1322])),\n",
       "             ('d.d_layers.2.ffns.linear2.weight',\n",
       "              tensor([[-0.0179, -0.0154, -0.0121,  ..., -0.0265, -0.0185, -0.0230],\n",
       "                      [ 0.0102,  0.0245,  0.0696,  ..., -0.0655,  0.0690, -0.0257],\n",
       "                      [ 0.0441, -0.0594, -0.0022,  ...,  0.0462,  0.0473,  0.0727],\n",
       "                      ...,\n",
       "                      [-0.0716,  0.0544,  0.0204,  ...,  0.0453,  0.0053,  0.0285],\n",
       "                      [-0.0250, -0.0231,  0.0367,  ...,  0.0355, -0.0394, -0.0462],\n",
       "                      [-0.0030, -0.0017, -0.0376,  ..., -0.0309,  0.0203,  0.0081]])),\n",
       "             ('d.d_layers.2.ffns.linear2.bias',\n",
       "              tensor([-0.0820,  0.0978, -0.0692,  ...,  0.1123, -0.1595,  0.0350])),\n",
       "             ('d.d_layers.2.layer_norm1.weight',\n",
       "              tensor([0.4768, 0.4742, 0.2371,  ..., 0.5040, 0.5466, 0.5715])),\n",
       "             ('d.d_layers.2.layer_norm1.bias',\n",
       "              tensor([ 0.0302,  0.0957,  0.0438,  ...,  0.0707, -0.0195,  0.0667])),\n",
       "             ('d.d_layers.2.layer_norm2.weight',\n",
       "              tensor([0.3681, 0.3524, 0.2329,  ..., 0.3809, 0.3719, 0.3847])),\n",
       "             ('d.d_layers.2.layer_norm2.bias',\n",
       "              tensor([ 0.0094, -0.0063,  0.0116,  ...,  0.0081,  0.0325,  0.0089])),\n",
       "             ('d.d_layers.3.attention.linearQ.weight',\n",
       "              tensor([[-0.0986,  0.0676,  0.0609,  ..., -0.0022,  0.1726,  0.0367],\n",
       "                      [ 0.1591,  0.0678,  0.0429,  ...,  0.0129, -0.0004,  0.0308],\n",
       "                      [ 0.0271,  0.0512, -0.0735,  ..., -0.0903, -0.0229,  0.0319],\n",
       "                      ...,\n",
       "                      [ 0.0023,  0.0193,  0.0217,  ..., -0.0349, -0.0140, -0.0679],\n",
       "                      [-0.0215, -0.1003,  0.0572,  ...,  0.0413, -0.2454,  0.0465],\n",
       "                      [-0.0418, -0.0130, -0.1526,  ...,  0.1621,  0.0046, -0.0616]])),\n",
       "             ('d.d_layers.3.attention.linearQ.bias',\n",
       "              tensor([-0.0961,  0.0035, -0.0896,  ...,  0.0112, -0.3032,  0.1218])),\n",
       "             ('d.d_layers.3.attention.linearK.weight',\n",
       "              tensor([[ 0.0228, -0.0457,  0.0192,  ..., -0.0496, -0.1007,  0.0120],\n",
       "                      [-0.2532, -0.0131,  0.1818,  ..., -0.0654,  0.0049, -0.0045],\n",
       "                      [-0.0100, -0.1318,  0.1282,  ...,  0.0726,  0.0973,  0.0507],\n",
       "                      ...,\n",
       "                      [ 0.0160, -0.1095, -0.0528,  ...,  0.0390,  0.0122, -0.0233],\n",
       "                      [ 0.0030,  0.0899, -0.0269,  ..., -0.0169, -0.1134, -0.0585],\n",
       "                      [ 0.1122, -0.0940,  0.0716,  ..., -0.0474, -0.0773,  0.1736]])),\n",
       "             ('d.d_layers.3.attention.linearK.bias',\n",
       "              tensor([-0.0183,  0.0041, -0.0290,  ...,  0.0175,  0.0861,  0.0065])),\n",
       "             ('d.d_layers.3.attention.linearV.weight',\n",
       "              tensor([[ 0.0100,  0.0226,  0.0209,  ..., -0.0076,  0.0332,  0.0712],\n",
       "                      [ 0.0202,  0.0029,  0.0223,  ...,  0.0139,  0.0496, -0.0055],\n",
       "                      [ 0.0816, -0.0211, -0.0502,  ..., -0.0487, -0.0500, -0.0600],\n",
       "                      ...,\n",
       "                      [ 0.0336,  0.0081,  0.0722,  ..., -0.1011, -0.0272, -0.0117],\n",
       "                      [-0.0426, -0.0431,  0.0081,  ..., -0.0267, -0.0214,  0.0029],\n",
       "                      [ 0.0278, -0.0229,  0.0423,  ..., -0.0403,  0.0177,  0.0294]])),\n",
       "             ('d.d_layers.3.attention.linearV.bias',\n",
       "              tensor([-0.0352, -0.0183,  0.0004,  ..., -0.0107,  0.0209, -0.0248])),\n",
       "             ('d.d_layers.3.attention.out.weight',\n",
       "              tensor([[-0.0477, -0.0002,  0.0530,  ..., -0.0448, -0.0024, -0.0513],\n",
       "                      [ 0.0523,  0.0267, -0.0082,  ...,  0.0020,  0.0358, -0.0498],\n",
       "                      [ 0.0235, -0.0517, -0.0506,  ...,  0.0164, -0.0232, -0.0070],\n",
       "                      ...,\n",
       "                      [-0.0034, -0.0119, -0.0005,  ..., -0.0271,  0.0253, -0.0172],\n",
       "                      [ 0.0290,  0.0240, -0.0240,  ..., -0.0221, -0.0060,  0.0011],\n",
       "                      [ 0.0510, -0.0023,  0.0319,  ..., -0.0392,  0.0052,  0.0172]])),\n",
       "             ('d.d_layers.3.attention.out.bias',\n",
       "              tensor([-0.0004, -0.0107, -0.0041,  ..., -0.0241, -0.0124, -0.0184])),\n",
       "             ('d.d_layers.3.ffns.linear1.weight',\n",
       "              tensor([[-6.9625e-02, -1.6698e-01, -2.2679e-01,  ...,  2.1818e-02,\n",
       "                       -2.9215e-02, -8.5102e-02],\n",
       "                      [ 2.3410e-03, -7.1822e-02, -9.9518e-03,  ..., -6.6059e-03,\n",
       "                        5.9896e-03,  4.3905e-02],\n",
       "                      [-7.0741e-01,  3.5792e-01,  5.0494e-01,  ...,  2.6759e-02,\n",
       "                       -6.4477e-01, -1.9407e-01],\n",
       "                      ...,\n",
       "                      [ 1.3251e-02, -1.4132e-02,  8.8962e-02,  ...,  2.1511e-02,\n",
       "                        2.7720e-02,  2.9540e-02],\n",
       "                      [-3.0268e-02, -3.9998e-04,  6.4605e-02,  ..., -3.8746e-02,\n",
       "                        3.0839e-02,  7.0653e-02],\n",
       "                      [ 2.4849e-02, -3.6046e-02, -2.1855e-02,  ...,  4.3703e-02,\n",
       "                        1.2780e-01, -8.3130e-03]])),\n",
       "             ('d.d_layers.3.ffns.linear1.bias',\n",
       "              tensor([-0.4254, -0.2217, -0.4353,  ...,  0.0726, -0.0057, -0.2265])),\n",
       "             ('d.d_layers.3.ffns.linear2.weight',\n",
       "              tensor([[-0.0087,  0.0155,  0.0261,  ...,  0.0532, -0.0169, -0.0326],\n",
       "                      [-0.0572, -0.0170, -0.0075,  ...,  0.0084,  0.0432, -0.0525],\n",
       "                      [-0.0211, -0.0532,  0.0041,  ...,  0.0414,  0.0471,  0.0112],\n",
       "                      ...,\n",
       "                      [-0.0852,  0.0935,  0.0130,  ..., -0.0170,  0.0343, -0.0529],\n",
       "                      [-0.0656, -0.0346,  0.0122,  ...,  0.0175,  0.0088,  0.0683],\n",
       "                      [-0.0739, -0.0112, -0.0120,  ...,  0.0344, -0.0371, -0.0204]])),\n",
       "             ('d.d_layers.3.ffns.linear2.bias',\n",
       "              tensor([-0.1074,  0.0315,  0.0992,  ...,  0.0824, -0.0823,  0.0817])),\n",
       "             ('d.d_layers.3.layer_norm1.weight',\n",
       "              tensor([0.4364, 0.4422, 0.1895,  ..., 0.4828, 0.5303, 0.5440])),\n",
       "             ('d.d_layers.3.layer_norm1.bias',\n",
       "              tensor([0.0910, 0.0779, 0.0233,  ..., 0.1450, 0.0090, 0.0678])),\n",
       "             ('d.d_layers.3.layer_norm2.weight',\n",
       "              tensor([0.3507, 0.3377, 0.2421,  ..., 0.3512, 0.3481, 0.3586])),\n",
       "             ('d.d_layers.3.layer_norm2.bias',\n",
       "              tensor([-0.0080, -0.0066,  0.0078,  ..., -0.0162,  0.0304,  0.0046])),\n",
       "             ('d.d_layers.4.attention.linearQ.weight',\n",
       "              tensor([[-0.0044,  0.0102,  0.0674,  ..., -0.0358,  0.0065, -0.0406],\n",
       "                      [ 0.1437,  0.0385,  0.0739,  ..., -0.0539,  0.0343,  0.1382],\n",
       "                      [ 0.1419, -0.0896,  0.1168,  ..., -0.0088,  0.0408,  0.0083],\n",
       "                      ...,\n",
       "                      [-0.0470, -0.1338,  0.1144,  ...,  0.1082, -0.0097, -0.1623],\n",
       "                      [-0.1064, -0.0884, -0.0754,  ..., -0.1307, -0.0355, -0.1175],\n",
       "                      [-0.0346,  0.0433,  0.0810,  ...,  0.1477,  0.1068, -0.0816]])),\n",
       "             ('d.d_layers.4.attention.linearQ.bias',\n",
       "              tensor([-0.0208, -0.1714,  0.1830,  ...,  0.1395, -0.1062,  0.3165])),\n",
       "             ('d.d_layers.4.attention.linearK.weight',\n",
       "              tensor([[ 0.1057,  0.0555,  0.0465,  ...,  0.0923,  0.1952, -0.0868],\n",
       "                      [ 0.0043, -0.1363, -0.0451,  ..., -0.0364, -0.0206, -0.0834],\n",
       "                      [-0.1063, -0.0111, -0.0738,  ...,  0.0732,  0.0467,  0.2091],\n",
       "                      ...,\n",
       "                      [-0.0857,  0.0307, -0.0029,  ...,  0.0394,  0.0312, -0.0444],\n",
       "                      [ 0.0094, -0.1128,  0.1751,  ..., -0.1277, -0.1753, -0.0409],\n",
       "                      [ 0.0233, -0.1867,  0.0131,  ..., -0.0528,  0.0665,  0.0181]])),\n",
       "             ('d.d_layers.4.attention.linearK.bias',\n",
       "              tensor([-0.0020,  0.0318,  0.0296,  ...,  0.0141,  0.0087,  0.0330])),\n",
       "             ('d.d_layers.4.attention.linearV.weight',\n",
       "              tensor([[ 2.0351e-03,  4.1648e-02,  1.6443e-01,  ..., -1.5152e-01,\n",
       "                        1.9675e-02,  4.1501e-02],\n",
       "                      [ 1.1681e-01, -3.2705e-02,  1.5265e-02,  ...,  9.6724e-02,\n",
       "                        4.1497e-06,  3.6233e-02],\n",
       "                      [ 4.1868e-03,  1.9621e-02, -1.0358e-01,  ..., -3.2099e-03,\n",
       "                        3.6373e-02,  6.9405e-02],\n",
       "                      ...,\n",
       "                      [ 2.3999e-02,  2.7746e-02,  6.4934e-03,  ...,  9.2423e-03,\n",
       "                        3.4963e-02,  5.3026e-02],\n",
       "                      [ 9.2460e-03, -4.0338e-02, -1.6015e-02,  ...,  2.2040e-02,\n",
       "                        7.5845e-02, -5.3213e-02],\n",
       "                      [-5.7953e-02,  4.4128e-02, -2.8196e-02,  ..., -1.8137e-02,\n",
       "                        7.3454e-02,  2.5076e-02]])),\n",
       "             ('d.d_layers.4.attention.linearV.bias',\n",
       "              tensor([ 0.0525, -0.0300,  0.0137,  ...,  0.0540,  0.0079, -0.0395])),\n",
       "             ('d.d_layers.4.attention.out.weight',\n",
       "              tensor([[-0.0033, -0.0339,  0.0154,  ...,  0.0063, -0.0087, -0.0076],\n",
       "                      [ 0.0053, -0.0445,  0.0399,  ..., -0.0065, -0.0241, -0.0473],\n",
       "                      [-0.0214,  0.0095, -0.0330,  ...,  0.0024, -0.0080, -0.0429],\n",
       "                      ...,\n",
       "                      [ 0.0247,  0.0160, -0.0534,  ..., -0.0053, -0.0235,  0.0115],\n",
       "                      [-0.0237, -0.0181,  0.0521,  ..., -0.0380, -0.0486, -0.0229],\n",
       "                      [-0.0021, -0.0300, -0.0107,  ...,  0.0504, -0.0339, -0.0377]])),\n",
       "             ('d.d_layers.4.attention.out.bias',\n",
       "              tensor([-0.0004, -0.0107, -0.0041,  ..., -0.0241, -0.0124, -0.0184])),\n",
       "             ('d.d_layers.4.ffns.linear1.weight',\n",
       "              tensor([[ 0.0216,  0.0751, -0.0569,  ..., -0.0129, -0.0270, -0.0024],\n",
       "                      [ 0.0344, -0.0152, -0.0054,  ..., -0.0230, -0.0335,  0.0385],\n",
       "                      [-0.0698,  0.0146, -0.0915,  ..., -0.0228,  0.0406, -0.0791],\n",
       "                      ...,\n",
       "                      [-0.0776, -0.0350,  0.0499,  ..., -0.0129, -0.1249, -0.0452],\n",
       "                      [ 0.0522, -0.0472, -0.0136,  ...,  0.0466, -0.0526, -0.0050],\n",
       "                      [ 0.0223,  0.0142,  0.0422,  ..., -0.0072, -0.0450,  0.0151]])),\n",
       "             ('d.d_layers.4.ffns.linear1.bias',\n",
       "              tensor([ 0.0089, -0.2012, -0.1557,  ..., -0.1270,  0.0273, -0.1238])),\n",
       "             ('d.d_layers.4.ffns.linear2.weight',\n",
       "              tensor([[ 0.0040, -0.0721, -0.0324,  ..., -0.0864, -0.0166, -0.0040],\n",
       "                      [-0.0312,  0.0026,  0.0074,  ...,  0.0431,  0.0127,  0.0259],\n",
       "                      [ 0.0513,  0.0423, -0.0184,  ..., -0.0292,  0.1080, -0.0010],\n",
       "                      ...,\n",
       "                      [ 0.0277,  0.0695, -0.0328,  ..., -0.0380,  0.0293,  0.0162],\n",
       "                      [ 0.0191, -0.0428, -0.0040,  ..., -0.0354,  0.0069,  0.0034],\n",
       "                      [ 0.0196, -0.0226, -0.0319,  ..., -0.0722, -0.0328, -0.0495]])),\n",
       "             ('d.d_layers.4.ffns.linear2.bias',\n",
       "              tensor([-0.0305,  0.0043, -0.0613,  ..., -0.0653, -0.3320, -0.0215])),\n",
       "             ('d.d_layers.4.layer_norm1.weight',\n",
       "              tensor([0.4332, 0.4392, 0.2770,  ..., 0.4634, 0.5124, 0.5299])),\n",
       "             ('d.d_layers.4.layer_norm1.bias',\n",
       "              tensor([ 0.0232,  0.1131,  0.0252,  ...,  0.0516, -0.0252,  0.0351])),\n",
       "             ('d.d_layers.4.layer_norm2.weight',\n",
       "              tensor([0.4054, 0.4021, 0.3370,  ..., 0.4128, 0.4009, 0.4142])),\n",
       "             ('d.d_layers.4.layer_norm2.bias',\n",
       "              tensor([ 0.0116, -0.0017,  0.0173,  ...,  0.0156,  0.0524,  0.0232])),\n",
       "             ('d.d_layers.5.attention.linearQ.weight',\n",
       "              tensor([[ 0.0852,  0.1012,  0.0978,  ..., -0.0792, -0.0244, -0.0164],\n",
       "                      [ 0.0170,  0.0793, -0.0351,  ...,  0.1482,  0.0174,  0.0707],\n",
       "                      [ 0.0941,  0.1224, -0.0831,  ...,  0.0243, -0.0296,  0.0639],\n",
       "                      ...,\n",
       "                      [ 0.0254, -0.0403,  0.0097,  ...,  0.1160,  0.0238,  0.0762],\n",
       "                      [-0.0349, -0.0175, -0.0616,  ...,  0.1488,  0.1092,  0.0140],\n",
       "                      [-0.0502, -0.0385,  0.0042,  ..., -0.0733,  0.0389, -0.0358]])),\n",
       "             ('d.d_layers.5.attention.linearQ.bias',\n",
       "              tensor([ 0.1946,  0.1501,  0.1053,  ...,  0.3054, -0.0996,  0.0542])),\n",
       "             ('d.d_layers.5.attention.linearK.weight',\n",
       "              tensor([[ 0.0780, -0.0202,  0.0056,  ...,  0.0462,  0.0197, -0.1847],\n",
       "                      [ 0.0495,  0.0496,  0.0216,  ...,  0.0495,  0.0752,  0.1669],\n",
       "                      [-0.0696,  0.0283,  0.0652,  ...,  0.0274, -0.1012, -0.1302],\n",
       "                      ...,\n",
       "                      [ 0.0532, -0.0071, -0.0907,  ...,  0.0028, -0.2750, -0.1537],\n",
       "                      [ 0.1811,  0.0298, -0.0234,  ...,  0.0566,  0.1626, -0.1257],\n",
       "                      [ 0.0071, -0.0978,  0.1203,  ..., -0.0103,  0.0959,  0.0261]])),\n",
       "             ('d.d_layers.5.attention.linearK.bias',\n",
       "              tensor([ 0.0477,  0.0353,  0.0635,  ..., -0.0364,  0.0408,  0.0443])),\n",
       "             ('d.d_layers.5.attention.linearV.weight',\n",
       "              tensor([[-0.0494, -0.0009, -0.0096,  ..., -0.0449,  0.0041, -0.0074],\n",
       "                      [ 0.0673, -0.0359, -0.0293,  ..., -0.0460, -0.0272, -0.1125],\n",
       "                      [ 0.0527, -0.0347, -0.0091,  ...,  0.0191,  0.0048,  0.0006],\n",
       "                      ...,\n",
       "                      [ 0.0049,  0.0336,  0.0153,  ...,  0.0911, -0.0442,  0.0479],\n",
       "                      [ 0.0337,  0.0355,  0.0384,  ..., -0.0281,  0.0313, -0.0204],\n",
       "                      [ 0.0575, -0.0180, -0.0780,  ...,  0.0264, -0.0920,  0.0188]])),\n",
       "             ('d.d_layers.5.attention.linearV.bias',\n",
       "              tensor([-0.0462, -0.0395,  0.0189,  ...,  0.0251,  0.0747, -0.0431])),\n",
       "             ('d.d_layers.5.attention.out.weight',\n",
       "              tensor([[ 0.0128, -0.0207,  0.0239,  ..., -0.0255,  0.0531, -0.0501],\n",
       "                      [ 0.0284, -0.0234,  0.0328,  ...,  0.0136, -0.0182,  0.0086],\n",
       "                      [ 0.0274,  0.0045, -0.0329,  ...,  0.0267, -0.0156, -0.0447],\n",
       "                      ...,\n",
       "                      [ 0.0258,  0.0256, -0.0440,  ..., -0.0067,  0.0215, -0.0344],\n",
       "                      [ 0.0503,  0.0413,  0.0062,  ...,  0.0514,  0.0474,  0.0302],\n",
       "                      [-0.0071,  0.0214,  0.0083,  ...,  0.0453, -0.0427, -0.0356]])),\n",
       "             ('d.d_layers.5.attention.out.bias',\n",
       "              tensor([-0.0004, -0.0107, -0.0041,  ..., -0.0241, -0.0124, -0.0184])),\n",
       "             ('d.d_layers.5.ffns.linear1.weight',\n",
       "              tensor([[-0.0100, -0.0504,  0.0171,  ...,  0.0218, -0.0028,  0.0202],\n",
       "                      [ 0.0391, -0.0372,  0.0385,  ..., -0.0062,  0.0111, -0.0325],\n",
       "                      [-0.0122, -0.0170, -0.0189,  ...,  0.0126,  0.0046, -0.0322],\n",
       "                      ...,\n",
       "                      [ 0.0286,  0.0261,  0.0013,  ..., -0.0159, -0.0136, -0.0088],\n",
       "                      [ 0.0498, -0.0212, -0.0348,  ...,  0.0263, -0.0318, -0.0394],\n",
       "                      [ 0.0171, -0.0505, -0.0144,  ...,  0.0146,  0.0050,  0.0124]])),\n",
       "             ('d.d_layers.5.ffns.linear1.bias',\n",
       "              tensor([-0.1056, -0.1680, -0.0700,  ..., -0.0004, -0.0529, -0.1539])),\n",
       "             ('d.d_layers.5.ffns.linear2.weight',\n",
       "              tensor([[ 0.0051,  0.0295,  0.0540,  ...,  0.0090,  0.0325,  0.0044],\n",
       "                      [ 0.0209, -0.0039, -0.0026,  ..., -0.0451,  0.0199,  0.0157],\n",
       "                      [ 0.0493, -0.0232, -0.0652,  ..., -0.0930, -0.0213,  0.0274],\n",
       "                      ...,\n",
       "                      [ 0.0203,  0.0559, -0.0607,  ..., -0.0402, -0.0022, -0.0680],\n",
       "                      [-0.0132, -0.0096, -0.0482,  ..., -0.0335, -0.0140,  0.0189],\n",
       "                      [-0.0652, -0.0032,  0.0133,  ..., -0.0196, -0.0393, -0.0709]])),\n",
       "             ('d.d_layers.5.ffns.linear2.bias',\n",
       "              tensor([ 0.0816,  0.0328, -0.0995,  ..., -0.0380, -0.1476, -0.0716])),\n",
       "             ('d.d_layers.5.layer_norm1.weight',\n",
       "              tensor([0.4685, 0.4853, 0.3902,  ..., 0.4730, 0.5013, 0.5023])),\n",
       "             ('d.d_layers.5.layer_norm1.bias',\n",
       "              tensor([ 0.0401,  0.0435,  0.0114,  ...,  0.0150, -0.0327,  0.0138])),\n",
       "             ('d.d_layers.5.layer_norm2.weight',\n",
       "              tensor([1.1205, 1.0961, 0.8306,  ..., 1.0240, 1.0755, 1.0793])),\n",
       "             ('d.d_layers.5.layer_norm2.bias',\n",
       "              tensor([ 0.0387, -0.0461, -0.1217,  ..., -0.0325, -0.0772, -0.0336])),\n",
       "             ('linear_f.weight',\n",
       "              tensor([[ 0.0307, -0.0386, -0.0738,  ...,  0.0267,  0.1240,  0.0797],\n",
       "                      [-0.0644,  0.0018, -0.0988,  ..., -0.0186,  0.0353, -0.0341],\n",
       "                      [-0.0066, -0.0235,  0.0923,  ..., -0.0092,  0.0856, -0.0017],\n",
       "                      ...,\n",
       "                      [-0.0865, -0.0261,  0.0752,  ..., -0.0530,  0.0849,  0.0014],\n",
       "                      [-0.0526, -0.0394, -0.1161,  ..., -0.0489,  0.1015, -0.0498],\n",
       "                      [-0.1617, -0.0334, -0.0070,  ...,  0.0039, -0.0352,  0.0359]])),\n",
       "             ('linear_f.bias',\n",
       "              tensor([-6.5581,  2.1886, -0.2786,  ..., -6.2638, -6.3265, -6.9061])),\n",
       "             ('e.e_layers.0.attention.linear.weight',\n",
       "              tensor([[-0.0258,  0.0107, -0.0602,  ..., -0.0161, -0.0465, -0.0123],\n",
       "                      [ 0.0150, -0.0013,  0.0058,  ...,  0.0157, -0.0349,  0.0101],\n",
       "                      [ 0.0708, -0.0037, -0.0628,  ...,  0.0247,  0.0669, -0.0720],\n",
       "                      ...,\n",
       "                      [-0.0261, -0.0426, -0.0486,  ..., -0.0219, -0.0367, -0.0104],\n",
       "                      [-0.0442,  0.0080,  0.0318,  ...,  0.0141,  0.0152, -0.0310],\n",
       "                      [-0.0011,  0.0270, -0.0167,  ..., -0.0135,  0.0286,  0.0120]])),\n",
       "             ('e.e_layers.0.attention.linear.bias',\n",
       "              tensor([-0.1001,  0.0321, -0.4002,  ..., -0.0177, -0.0323,  0.0184])),\n",
       "             ('e.e_layers.1.attention.linear.weight',\n",
       "              tensor([[-5.5464e-02, -2.7033e-02, -6.1285e-02,  ..., -4.4741e-02,\n",
       "                        5.9505e-03, -3.8018e-03],\n",
       "                      [ 4.0400e-02, -2.5832e-03, -1.0927e-02,  ...,  3.7821e-02,\n",
       "                        1.7592e-02, -5.2732e-03],\n",
       "                      [-5.0723e-02, -7.7763e-02,  7.5126e-02,  ...,  6.2292e-05,\n",
       "                        1.6274e-02, -5.5656e-02],\n",
       "                      ...,\n",
       "                      [ 1.4572e-02,  1.3093e-02, -3.3897e-02,  ...,  1.1195e-02,\n",
       "                       -9.2095e-02, -1.0533e-02],\n",
       "                      [ 3.1864e-04,  3.7674e-02,  3.4483e-02,  ...,  9.2445e-03,\n",
       "                        4.1908e-02,  2.8672e-02],\n",
       "                      [-1.4554e-02,  3.4598e-02,  4.9582e-03,  ..., -6.8392e-04,\n",
       "                       -1.3266e-02, -1.3335e-02]])),\n",
       "             ('e.e_layers.1.attention.linear.bias',\n",
       "              tensor([-0.0642, -0.0629,  0.1401,  ..., -0.0548, -0.0366,  0.0493])),\n",
       "             ('e.e_layers.2.attention.linear.weight',\n",
       "              tensor([[-0.0435,  0.0221,  0.0421,  ..., -0.0308,  0.0463, -0.0796],\n",
       "                      [-0.0192,  0.0312, -0.0242,  ..., -0.0348, -0.0308, -0.0124],\n",
       "                      [-0.0398, -0.1044, -0.0293,  ..., -0.0867, -0.1216, -0.0404],\n",
       "                      ...,\n",
       "                      [-0.0016, -0.0451, -0.0122,  ..., -0.0403, -0.0346,  0.0205],\n",
       "                      [-0.0381,  0.0041,  0.0023,  ..., -0.0457,  0.0239,  0.0092],\n",
       "                      [ 0.0049,  0.0140,  0.0117,  ...,  0.0167, -0.0146,  0.0257]])),\n",
       "             ('e.e_layers.2.attention.linear.bias',\n",
       "              tensor([ 0.0346, -0.1537,  0.1721,  ..., -0.0002,  0.1398, -0.0273])),\n",
       "             ('e.e_layers.3.attention.linear.weight',\n",
       "              tensor([[-0.0205,  0.0336, -0.0080,  ..., -0.0437,  0.0334,  0.0775],\n",
       "                      [-0.0220, -0.0093,  0.0407,  ..., -0.0677, -0.0071, -0.0052],\n",
       "                      [-0.0210, -0.0519,  0.0369,  ...,  0.0508,  0.1250, -0.0551],\n",
       "                      ...,\n",
       "                      [-0.0185,  0.0176, -0.0398,  ..., -0.0155, -0.0150, -0.0085],\n",
       "                      [-0.0151,  0.0130, -0.0355,  ..., -0.0294, -0.0036,  0.0184],\n",
       "                      [-0.0645,  0.0032,  0.0569,  ...,  0.0509,  0.0334,  0.0343]])),\n",
       "             ('e.e_layers.3.attention.linear.bias',\n",
       "              tensor([-0.0045, -0.1039,  0.0208,  ..., -0.0433,  0.1130, -0.0501])),\n",
       "             ('e.e_layers.4.attention.linear.weight',\n",
       "              tensor([[ 0.0154, -0.1323, -0.0074,  ...,  0.0017, -0.0296, -0.0310],\n",
       "                      [-0.0292,  0.0330,  0.0096,  ..., -0.0052,  0.0033, -0.0027],\n",
       "                      [ 0.0426,  0.0312, -0.0545,  ...,  0.0425,  0.0122,  0.0070],\n",
       "                      ...,\n",
       "                      [ 0.0615, -0.0751, -0.0234,  ...,  0.0147, -0.0010, -0.0076],\n",
       "                      [-0.0326, -0.0143, -0.0271,  ..., -0.0211,  0.0082, -0.0332],\n",
       "                      [ 0.0021,  0.0015, -0.0046,  ...,  0.0485,  0.0134,  0.0092]])),\n",
       "             ('e.e_layers.4.attention.linear.bias',\n",
       "              tensor([ 0.0658, -0.1263, -0.0467,  ..., -0.0601, -0.0051,  0.0272])),\n",
       "             ('e.e_layers.5.attention.linear.weight',\n",
       "              tensor([[-0.0605,  0.0521,  0.0341,  ..., -0.0065,  0.0360,  0.1002],\n",
       "                      [ 0.0115,  0.0049, -0.0437,  ...,  0.0331,  0.0650, -0.0059],\n",
       "                      [ 0.0598,  0.0204, -0.0272,  ..., -0.0333, -0.0851,  0.0147],\n",
       "                      ...,\n",
       "                      [ 0.0357,  0.0143,  0.0555,  ...,  0.0991, -0.0147,  0.0735],\n",
       "                      [ 0.0313, -0.0521, -0.0164,  ..., -0.0572, -0.0039, -0.0460],\n",
       "                      [ 0.0059,  0.0062, -0.0037,  ...,  0.0365,  0.0275,  0.0966]])),\n",
       "             ('e.e_layers.5.attention.linear.bias',\n",
       "              tensor([ 0.0406, -0.0371,  0.0054,  ...,  0.0396,  0.0514,  0.0095])),\n",
       "             ('d.d_layers.0.attention.linear.weight',\n",
       "              tensor([[-0.0258,  0.0107, -0.0602,  ..., -0.0161, -0.0465, -0.0123],\n",
       "                      [ 0.0150, -0.0013,  0.0058,  ...,  0.0157, -0.0349,  0.0101],\n",
       "                      [ 0.0708, -0.0037, -0.0628,  ...,  0.0247,  0.0669, -0.0720],\n",
       "                      ...,\n",
       "                      [-0.0261, -0.0426, -0.0486,  ..., -0.0219, -0.0367, -0.0104],\n",
       "                      [-0.0442,  0.0080,  0.0318,  ...,  0.0141,  0.0152, -0.0310],\n",
       "                      [-0.0011,  0.0270, -0.0167,  ..., -0.0135,  0.0286,  0.0120]])),\n",
       "             ('d.d_layers.0.attention.linear.bias',\n",
       "              tensor([-0.1001,  0.0321, -0.4002,  ..., -0.0177, -0.0323,  0.0184])),\n",
       "             ('d.d_layers.1.attention.linear.weight',\n",
       "              tensor([[-5.5464e-02, -2.7033e-02, -6.1285e-02,  ..., -4.4741e-02,\n",
       "                        5.9505e-03, -3.8018e-03],\n",
       "                      [ 4.0400e-02, -2.5832e-03, -1.0927e-02,  ...,  3.7821e-02,\n",
       "                        1.7592e-02, -5.2732e-03],\n",
       "                      [-5.0723e-02, -7.7763e-02,  7.5126e-02,  ...,  6.2292e-05,\n",
       "                        1.6274e-02, -5.5656e-02],\n",
       "                      ...,\n",
       "                      [ 1.4572e-02,  1.3093e-02, -3.3897e-02,  ...,  1.1195e-02,\n",
       "                       -9.2095e-02, -1.0533e-02],\n",
       "                      [ 3.1864e-04,  3.7674e-02,  3.4483e-02,  ...,  9.2445e-03,\n",
       "                        4.1908e-02,  2.8672e-02],\n",
       "                      [-1.4554e-02,  3.4598e-02,  4.9582e-03,  ..., -6.8392e-04,\n",
       "                       -1.3266e-02, -1.3335e-02]])),\n",
       "             ('d.d_layers.1.attention.linear.bias',\n",
       "              tensor([-0.0642, -0.0629,  0.1401,  ..., -0.0548, -0.0366,  0.0493])),\n",
       "             ('d.d_layers.2.attention.linear.weight',\n",
       "              tensor([[-0.0435,  0.0221,  0.0421,  ..., -0.0308,  0.0463, -0.0796],\n",
       "                      [-0.0192,  0.0312, -0.0242,  ..., -0.0348, -0.0308, -0.0124],\n",
       "                      [-0.0398, -0.1044, -0.0293,  ..., -0.0867, -0.1216, -0.0404],\n",
       "                      ...,\n",
       "                      [-0.0016, -0.0451, -0.0122,  ..., -0.0403, -0.0346,  0.0205],\n",
       "                      [-0.0381,  0.0041,  0.0023,  ..., -0.0457,  0.0239,  0.0092],\n",
       "                      [ 0.0049,  0.0140,  0.0117,  ...,  0.0167, -0.0146,  0.0257]])),\n",
       "             ('d.d_layers.2.attention.linear.bias',\n",
       "              tensor([ 0.0346, -0.1537,  0.1721,  ..., -0.0002,  0.1398, -0.0273])),\n",
       "             ('d.d_layers.3.attention.linear.weight',\n",
       "              tensor([[-0.0205,  0.0336, -0.0080,  ..., -0.0437,  0.0334,  0.0775],\n",
       "                      [-0.0220, -0.0093,  0.0407,  ..., -0.0677, -0.0071, -0.0052],\n",
       "                      [-0.0210, -0.0519,  0.0369,  ...,  0.0508,  0.1250, -0.0551],\n",
       "                      ...,\n",
       "                      [-0.0185,  0.0176, -0.0398,  ..., -0.0155, -0.0150, -0.0085],\n",
       "                      [-0.0151,  0.0130, -0.0355,  ..., -0.0294, -0.0036,  0.0184],\n",
       "                      [-0.0645,  0.0032,  0.0569,  ...,  0.0509,  0.0334,  0.0343]])),\n",
       "             ('d.d_layers.3.attention.linear.bias',\n",
       "              tensor([-0.0045, -0.1039,  0.0208,  ..., -0.0433,  0.1130, -0.0501])),\n",
       "             ('d.d_layers.4.attention.linear.weight',\n",
       "              tensor([[ 0.0154, -0.1323, -0.0074,  ...,  0.0017, -0.0296, -0.0310],\n",
       "                      [-0.0292,  0.0330,  0.0096,  ..., -0.0052,  0.0033, -0.0027],\n",
       "                      [ 0.0426,  0.0312, -0.0545,  ...,  0.0425,  0.0122,  0.0070],\n",
       "                      ...,\n",
       "                      [ 0.0615, -0.0751, -0.0234,  ...,  0.0147, -0.0010, -0.0076],\n",
       "                      [-0.0326, -0.0143, -0.0271,  ..., -0.0211,  0.0082, -0.0332],\n",
       "                      [ 0.0021,  0.0015, -0.0046,  ...,  0.0485,  0.0134,  0.0092]])),\n",
       "             ('d.d_layers.4.attention.linear.bias',\n",
       "              tensor([ 0.0658, -0.1263, -0.0467,  ..., -0.0601, -0.0051,  0.0272])),\n",
       "             ('d.d_layers.5.attention.linear.weight',\n",
       "              tensor([[-0.0605,  0.0521,  0.0341,  ..., -0.0065,  0.0360,  0.1002],\n",
       "                      [ 0.0115,  0.0049, -0.0437,  ...,  0.0331,  0.0650, -0.0059],\n",
       "                      [ 0.0598,  0.0204, -0.0272,  ..., -0.0333, -0.0851,  0.0147],\n",
       "                      ...,\n",
       "                      [ 0.0357,  0.0143,  0.0555,  ...,  0.0991, -0.0147,  0.0735],\n",
       "                      [ 0.0313, -0.0521, -0.0164,  ..., -0.0572, -0.0039, -0.0460],\n",
       "                      [ 0.0059,  0.0062, -0.0037,  ...,  0.0365,  0.0275,  0.0966]])),\n",
       "             ('d.d_layers.5.attention.linear.bias',\n",
       "              tensor([ 0.0406, -0.0371,  0.0054,  ...,  0.0396,  0.0514,  0.0095]))])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c424f903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
