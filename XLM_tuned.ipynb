{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d33bed",
   "metadata": {},
   "source": [
    "Skip to content\n",
    "Search or jump to…\n",
    "Pull requests\n",
    "Issues\n",
    "Marketplace\n",
    "Explore\n",
    " \n",
    "@Maplewarrior \n",
    "facebookresearch\n",
    "/\n",
    "XLM\n",
    "Public\n",
    "Code\n",
    "Issues\n",
    "112\n",
    "Pull requests\n",
    "10\n",
    "Actions\n",
    "Projects\n",
    "Security\n",
    "Insights\n",
    "XLM/xlm/model/transformer.py /\n",
    "@louismartin\n",
    "louismartin Setup package (#205)\n",
    "…\n",
    "Latest commit 0e1bde1 on 29 Mar 2021\n",
    " History\n",
    " 1 contributor\n",
    "Executable File  748 lines (617 sloc)  28.9 KB\n",
    "\n",
    "# Copyright (c) 2019-present, Facebook, Inc.\n",
    "# All rights reserved.\n",
    "#\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "420c3c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import getLogger\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from .memory import HashingMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4334b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "N_MAX_POSITIONS = 512  # maximum input sequence length\n",
    "\n",
    "DECODER_ONLY_PARAMS = [\n",
    "    'layer_norm15.%i.weight', 'layer_norm15.%i.bias',\n",
    "    'encoder_attn.%i.q_lin.weight', 'encoder_attn.%i.q_lin.bias',\n",
    "    'encoder_attn.%i.k_lin.weight', 'encoder_attn.%i.k_lin.bias',\n",
    "    'encoder_attn.%i.v_lin.weight', 'encoder_attn.%i.v_lin.bias',\n",
    "    'encoder_attn.%i.out_lin.weight', 'encoder_attn.%i.out_lin.bias'\n",
    "]\n",
    "\n",
    "TRANSFORMER_LAYER_PARAMS = [\n",
    "    'attentions.%i.q_lin.weight', 'attentions.%i.q_lin.bias',\n",
    "    'attentions.%i.k_lin.weight', 'attentions.%i.k_lin.bias',\n",
    "    'attentions.%i.v_lin.weight', 'attentions.%i.v_lin.bias',\n",
    "    'attentions.%i.out_lin.weight', 'attentions.%i.out_lin.bias',\n",
    "    'layer_norm1.%i.weight', 'layer_norm1.%i.bias',\n",
    "    'ffns.%i.lin1.weight', 'ffns.%i.lin1.bias',\n",
    "    'ffns.%i.lin2.weight', 'ffns.%i.lin2.bias',\n",
    "    'layer_norm2.%i.weight', 'layer_norm2.%i.bias'\n",
    "]\n",
    "\n",
    "\n",
    "logger = getLogger()\n",
    "\n",
    "\n",
    "def Embedding(num_embeddings, embedding_dim, padding_idx=None):\n",
    "    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "    nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)\n",
    "    if padding_idx is not None:\n",
    "        nn.init.constant_(m.weight[padding_idx], 0)\n",
    "    return m\n",
    "\n",
    "\n",
    "def Linear(in_features, out_features, bias=True):\n",
    "    m = nn.Linear(in_features, out_features, bias)\n",
    "    # nn.init.normal_(m.weight, mean=0, std=1)\n",
    "    # nn.init.xavier_uniform_(m.weight)\n",
    "    # nn.init.constant_(m.bias, 0.)\n",
    "    return m\n",
    "\n",
    "\n",
    "def create_sinusoidal_embeddings(n_pos, dim, out):\n",
    "    position_enc = np.array([\n",
    "        [pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n",
    "        for pos in range(n_pos)\n",
    "    ])\n",
    "    out[:, 0::2] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
    "    out[:, 1::2] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
    "    out.detach_()\n",
    "    out.requires_grad = False\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    GELU activation\n",
    "    https://arxiv.org/abs/1606.08415\n",
    "    https://github.com/huggingface/pytorch-openai-transformer-lm/blob/master/model_pytorch.py#L14\n",
    "    https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/modeling.py\n",
    "    \"\"\"\n",
    "    # return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    return 0.5 * x * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "def get_masks(slen, lengths, causal):\n",
    "    \"\"\"\n",
    "    Generate hidden states mask, and optionally an attention mask.\n",
    "    \"\"\"\n",
    "    assert lengths.max().item() <= slen\n",
    "    bs = lengths.size(0)\n",
    "    alen = torch.arange(slen, dtype=torch.long, device=lengths.device)\n",
    "    mask = alen < lengths[:, None]\n",
    "\n",
    "    # attention mask is the same as mask, or triangular inferior attention (causal)\n",
    "    if causal:\n",
    "        attn_mask = alen[None, None, :].repeat(bs, slen, 1) <= alen[None, :, None]\n",
    "    else:\n",
    "        attn_mask = mask\n",
    "\n",
    "    # sanity check\n",
    "    assert mask.size() == (bs, slen)\n",
    "    assert causal is False or attn_mask.size() == (bs, slen, slen)\n",
    "\n",
    "    return mask, attn_mask\n",
    "\n",
    "\n",
    "class PredLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Prediction layer (cross_entropy or adaptive_softmax).\n",
    "    \"\"\"\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.asm = params.asm\n",
    "        self.n_words = params.n_words\n",
    "        self.pad_index = params.pad_index\n",
    "        dim = params.emb_dim\n",
    "\n",
    "        if params.asm is False:\n",
    "            self.proj = Linear(dim, params.n_words, bias=True)\n",
    "        else:\n",
    "            self.proj = nn.AdaptiveLogSoftmaxWithLoss(\n",
    "                in_features=dim,\n",
    "                n_classes=params.n_words,\n",
    "                cutoffs=params.asm_cutoffs,\n",
    "                div_value=params.asm_div_value,\n",
    "                head_bias=True,  # default is False\n",
    "            )\n",
    "\n",
    "    def forward(self, x, y, get_scores=False):\n",
    "        \"\"\"\n",
    "        Compute the loss, and optionally the scores.\n",
    "        \"\"\"\n",
    "        assert (y == self.pad_index).sum().item() == 0\n",
    "\n",
    "        if self.asm is False:\n",
    "            scores = self.proj(x).view(-1, self.n_words)\n",
    "            loss = F.cross_entropy(scores, y, reduction='mean')\n",
    "        else:\n",
    "            _, loss = self.proj(x, y)\n",
    "            scores = self.proj.log_prob(x) if get_scores else None\n",
    "\n",
    "        return scores, loss\n",
    "\n",
    "    def get_scores(self, x):\n",
    "        \"\"\"\n",
    "        Compute scores.\n",
    "        \"\"\"\n",
    "        assert x.dim() == 2\n",
    "        return self.proj.log_prob(x) if self.asm else self.proj(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    NEW_ID = itertools.count()\n",
    "\n",
    "    def __init__(self, n_heads, dim, dropout):\n",
    "        super().__init__()\n",
    "        self.layer_id = next(MultiHeadAttention.NEW_ID)\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = dropout\n",
    "        assert self.dim % self.n_heads == 0\n",
    "\n",
    "        self.q_lin = Linear(dim, dim)\n",
    "        self.k_lin = Linear(dim, dim)\n",
    "        self.v_lin = Linear(dim, dim)\n",
    "        self.out_lin = Linear(dim, dim)\n",
    "\n",
    "    def forward(self, input, mask, kv=None, cache=None):\n",
    "        \"\"\"\n",
    "        Self-attention (if kv is None) or attention over source sentence (provided by kv).\n",
    "        \"\"\"\n",
    "        # Input is (bs, qlen, dim)\n",
    "        # Mask is (bs, klen) (non-causal) or (bs, klen, klen)\n",
    "        bs, qlen, dim = input.size()\n",
    "        if kv is None:\n",
    "            klen = qlen if cache is None else cache['slen'] + qlen\n",
    "        else:\n",
    "            klen = kv.size(1)\n",
    "        assert dim == self.dim, 'Dimensions do not match: %s input vs %s configured' % (dim, self.dim)\n",
    "        n_heads = self.n_heads\n",
    "        dim_per_head = dim // n_heads\n",
    "        mask_reshape = (bs, 1, qlen, klen) if mask.dim() == 3 else (bs, 1, 1, klen)\n",
    "\n",
    "        def shape(x):\n",
    "            \"\"\"  projection \"\"\"\n",
    "            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)\n",
    "\n",
    "        def unshape(x):\n",
    "            \"\"\"  compute context \"\"\"\n",
    "            return x.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * dim_per_head)\n",
    "\n",
    "        q = shape(self.q_lin(input))                                          # (bs, n_heads, qlen, dim_per_head)\n",
    "        if kv is None:\n",
    "            k = shape(self.k_lin(input))                                      # (bs, n_heads, qlen, dim_per_head)\n",
    "            v = shape(self.v_lin(input))                                      # (bs, n_heads, qlen, dim_per_head)\n",
    "        elif cache is None or self.layer_id not in cache:\n",
    "            k = v = kv\n",
    "            k = shape(self.k_lin(k))                                          # (bs, n_heads, qlen, dim_per_head)\n",
    "            v = shape(self.v_lin(v))                                          # (bs, n_heads, qlen, dim_per_head)\n",
    "\n",
    "        if cache is not None:\n",
    "            if self.layer_id in cache:\n",
    "                if kv is None:\n",
    "                    k_, v_ = cache[self.layer_id]\n",
    "                    k = torch.cat([k_, k], dim=2)                             # (bs, n_heads, klen, dim_per_head)\n",
    "                    v = torch.cat([v_, v], dim=2)                             # (bs, n_heads, klen, dim_per_head)\n",
    "                else:\n",
    "                    k, v = cache[self.layer_id]\n",
    "            cache[self.layer_id] = (k, v)\n",
    "\n",
    "        q = q / math.sqrt(dim_per_head)                                       # (bs, n_heads, qlen, dim_per_head)\n",
    "        scores = torch.matmul(q, k.transpose(2, 3))                           # (bs, n_heads, qlen, klen)\n",
    "        mask = (mask == 0).view(mask_reshape).expand_as(scores)               # (bs, n_heads, qlen, klen)\n",
    "        scores.masked_fill_(mask, -float('inf'))                              # (bs, n_heads, qlen, klen)\n",
    "\n",
    "        weights = F.softmax(scores.float(), dim=-1).type_as(scores)           # (bs, n_heads, qlen, klen)\n",
    "        weights = F.dropout(weights, p=self.dropout, training=self.training)  # (bs, n_heads, qlen, klen)\n",
    "        context = torch.matmul(weights, v)                                    # (bs, n_heads, qlen, dim_per_head)\n",
    "        context = unshape(context)                                            # (bs, qlen, dim)\n",
    "\n",
    "        return self.out_lin(context)\n",
    "\n",
    "\n",
    "class TransformerFFN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, dim_hidden, out_dim, dropout, gelu_activation):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.lin1 = Linear(in_dim, dim_hidden)\n",
    "        self.lin2 = Linear(dim_hidden, out_dim)\n",
    "        self.act = gelu if gelu_activation else F.relu\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.lin1(input)\n",
    "        x = self.act(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    ATTRIBUTES = ['encoder', 'with_output', 'eos_index', 'pad_index', 'n_langs', 'n_words', 'dim', 'n_layers', 'n_heads', 'hidden_dim', 'dropout', 'attention_dropout', 'asm', 'asm_cutoffs', 'asm_div_value']\n",
    "\n",
    "    def __init__(self, params, dico, is_encoder, with_output):\n",
    "        \"\"\"\n",
    "        Transformer model (encoder or decoder).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # encoder / decoder, output layer\n",
    "        self.is_encoder = is_encoder\n",
    "        self.is_decoder = not is_encoder\n",
    "        self.with_output = with_output\n",
    "\n",
    "        # dictionary / languages\n",
    "        self.n_langs = params.n_langs\n",
    "        self.n_words = params.n_words\n",
    "        self.eos_index = params.eos_index\n",
    "        self.pad_index = params.pad_index\n",
    "        self.dico = dico\n",
    "        self.id2lang = params.id2lang\n",
    "        self.lang2id = params.lang2id\n",
    "        self.use_lang_emb = getattr(params, 'use_lang_emb', True)\n",
    "        assert len(self.dico) == self.n_words\n",
    "        assert len(self.id2lang) == len(self.lang2id) == self.n_langs\n",
    "\n",
    "        # model parameters\n",
    "        self.dim = params.emb_dim       # 512 by default\n",
    "        self.hidden_dim = self.dim * 4  # 2048 by default\n",
    "        self.n_heads = params.n_heads   # 8 by default\n",
    "        self.n_layers = params.n_layers\n",
    "        self.dropout = params.dropout\n",
    "        self.attention_dropout = params.attention_dropout\n",
    "        assert self.dim % self.n_heads == 0, 'transformer dim must be a multiple of n_heads'\n",
    "\n",
    "        # embeddings\n",
    "        self.position_embeddings = Embedding(N_MAX_POSITIONS, self.dim)\n",
    "        if params.sinusoidal_embeddings:\n",
    "            create_sinusoidal_embeddings(N_MAX_POSITIONS, self.dim, out=self.position_embeddings.weight)\n",
    "        if params.n_langs > 1 and self.use_lang_emb:\n",
    "            self.lang_embeddings = Embedding(self.n_langs, self.dim)\n",
    "        self.embeddings = Embedding(self.n_words, self.dim, padding_idx=self.pad_index)\n",
    "        self.layer_norm_emb = nn.LayerNorm(self.dim, eps=1e-12)\n",
    "\n",
    "        # transformer layers\n",
    "        self.attentions = nn.ModuleList()\n",
    "        self.layer_norm1 = nn.ModuleList()\n",
    "        self.ffns = nn.ModuleList()\n",
    "        self.layer_norm2 = nn.ModuleList()\n",
    "        if self.is_decoder:\n",
    "            self.layer_norm15 = nn.ModuleList()\n",
    "            self.encoder_attn = nn.ModuleList()\n",
    "\n",
    "        # memories\n",
    "        self.memories = nn.ModuleDict()\n",
    "        if getattr(params, 'use_memory', False):\n",
    "            mem_positions = params.mem_enc_positions if is_encoder else params.mem_dec_positions\n",
    "            for layer_id, pos in mem_positions:\n",
    "                assert 0 <= layer_id <= params.n_layers - 1\n",
    "                assert pos in ['in', 'after']\n",
    "                self.memories['%i_%s' % (layer_id, pos)] = HashingMemory.build(self.dim, self.dim, params)\n",
    "\n",
    "        for layer_id in range(self.n_layers):\n",
    "            self.attentions.append(MultiHeadAttention(self.n_heads, self.dim, dropout=self.attention_dropout))\n",
    "            self.layer_norm1.append(nn.LayerNorm(self.dim, eps=1e-12))\n",
    "            if self.is_decoder:\n",
    "                self.layer_norm15.append(nn.LayerNorm(self.dim, eps=1e-12))\n",
    "                self.encoder_attn.append(MultiHeadAttention(self.n_heads, self.dim, dropout=self.attention_dropout))\n",
    "            if ('%i_in' % layer_id) in self.memories:\n",
    "                self.ffns.append(None)\n",
    "            else:\n",
    "                self.ffns.append(TransformerFFN(self.dim, self.hidden_dim, self.dim, dropout=self.dropout, gelu_activation=params.gelu_activation))\n",
    "            self.layer_norm2.append(nn.LayerNorm(self.dim, eps=1e-12))\n",
    "\n",
    "        # output layer\n",
    "        if self.with_output:\n",
    "            self.pred_layer = PredLayer(params)\n",
    "            if params.share_inout_emb:\n",
    "                self.pred_layer.proj.weight = self.embeddings.weight\n",
    "\n",
    "    def forward(self, mode, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward function with different forward modes.\n",
    "        ### Small hack to handle PyTorch distributed.\n",
    "        \"\"\"\n",
    "        if mode == 'fwd':\n",
    "            return self.fwd(**kwargs)\n",
    "        elif mode == 'predict':\n",
    "            return self.predict(**kwargs)\n",
    "        else:\n",
    "            raise Exception(\"Unknown mode: %s\" % mode)\n",
    "\n",
    "    def fwd(self, x, lengths, causal, src_enc=None, src_len=None, positions=None, langs=None, cache=None):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            `x` LongTensor(slen, bs), containing word indices\n",
    "            `lengths` LongTensor(bs), containing the length of each sentence\n",
    "            `causal` Boolean, if True, the attention is only done over previous hidden states\n",
    "            `positions` LongTensor(slen, bs), containing word positions\n",
    "            `langs` LongTensor(slen, bs), containing language IDs\n",
    "        \"\"\"\n",
    "        # lengths = (x != self.pad_index).float().sum(dim=1)\n",
    "        # mask = x != self.pad_index\n",
    "\n",
    "        # check inputs\n",
    "        slen, bs = x.size()\n",
    "        assert lengths.size(0) == bs\n",
    "        assert lengths.max().item() <= slen\n",
    "        x = x.transpose(0, 1)  # batch size as dimension 0\n",
    "        assert (src_enc is None) == (src_len is None)\n",
    "        if src_enc is not None:\n",
    "            assert self.is_decoder\n",
    "            assert src_enc.size(0) == bs\n",
    "\n",
    "        # generate masks\n",
    "        mask, attn_mask = get_masks(slen, lengths, causal)\n",
    "        if self.is_decoder and src_enc is not None:\n",
    "            src_mask = torch.arange(src_len.max(), dtype=torch.long, device=lengths.device) < src_len[:, None]\n",
    "\n",
    "        # positions\n",
    "        if positions is None:\n",
    "            positions = x.new(slen).long()\n",
    "            positions = torch.arange(slen, out=positions).unsqueeze(0)\n",
    "        else:\n",
    "            assert positions.size() == (slen, bs)\n",
    "            positions = positions.transpose(0, 1)\n",
    "\n",
    "        # langs\n",
    "        if langs is not None:\n",
    "            assert langs.size() == (slen, bs)\n",
    "            langs = langs.transpose(0, 1)\n",
    "\n",
    "        # do not recompute cached elements\n",
    "        if cache is not None:\n",
    "            _slen = slen - cache['slen']\n",
    "            x = x[:, -_slen:]\n",
    "            positions = positions[:, -_slen:]\n",
    "            if langs is not None:\n",
    "                langs = langs[:, -_slen:]\n",
    "            mask = mask[:, -_slen:]\n",
    "            attn_mask = attn_mask[:, -_slen:]\n",
    "\n",
    "        # embeddings\n",
    "        tensor = self.embeddings(x)\n",
    "        tensor = tensor + self.position_embeddings(positions).expand_as(tensor)\n",
    "        if langs is not None and self.use_lang_emb:\n",
    "            tensor = tensor + self.lang_embeddings(langs)\n",
    "        tensor = self.layer_norm_emb(tensor)\n",
    "        tensor = F.dropout(tensor, p=self.dropout, training=self.training)\n",
    "        tensor *= mask.unsqueeze(-1).to(tensor.dtype)\n",
    "\n",
    "        # transformer layers\n",
    "        for i in range(self.n_layers):\n",
    "\n",
    "            # self attention\n",
    "            attn = self.attentions[i](tensor, attn_mask, cache=cache)\n",
    "            attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
    "            tensor = tensor + attn\n",
    "            tensor = self.layer_norm1[i](tensor)\n",
    "\n",
    "            # encoder attention (for decoder only)\n",
    "            if self.is_decoder and src_enc is not None:\n",
    "                attn = self.encoder_attn[i](tensor, src_mask, kv=src_enc, cache=cache)\n",
    "                attn = F.dropout(attn, p=self.dropout, training=self.training)\n",
    "                tensor = tensor + attn\n",
    "                tensor = self.layer_norm15[i](tensor)\n",
    "\n",
    "            # FFN\n",
    "            if ('%i_in' % i) in self.memories:\n",
    "                tensor = tensor + self.memories['%i_in' % i](tensor)\n",
    "            else:\n",
    "                tensor = tensor + self.ffns[i](tensor)\n",
    "            tensor = self.layer_norm2[i](tensor)\n",
    "\n",
    "            # memory\n",
    "            if ('%i_after' % i) in self.memories:\n",
    "                tensor = tensor + self.memories['%i_after' % i](tensor)\n",
    "            # TODO: add extra layer norm here?\n",
    "\n",
    "            tensor *= mask.unsqueeze(-1).to(tensor.dtype)\n",
    "\n",
    "        # update cache length\n",
    "        if cache is not None:\n",
    "            cache['slen'] += tensor.size(1)\n",
    "\n",
    "        # move back sequence length to dimension 0\n",
    "        tensor = tensor.transpose(0, 1)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def predict(self, tensor, pred_mask, y, get_scores):\n",
    "        \"\"\"\n",
    "        Given the last hidden state, compute word scores and/or the loss.\n",
    "            `pred_mask` is a ByteTensor of shape (slen, bs), filled with 1 when\n",
    "                we need to predict a word\n",
    "            `y` is a LongTensor of shape (pred_mask.sum(),)\n",
    "            `get_scores` is a boolean specifying whether we need to return scores\n",
    "        \"\"\"\n",
    "        masked_tensor = tensor[pred_mask.unsqueeze(-1).expand_as(tensor)].view(-1, self.dim)\n",
    "        scores, loss = self.pred_layer(masked_tensor, y, get_scores)\n",
    "        return scores, loss\n",
    "\n",
    "    def generate(self, src_enc, src_len, tgt_lang_id, max_len=200, sample_temperature=None):\n",
    "        \"\"\"\n",
    "        Decode a sentence given initial start.\n",
    "        `x`:\n",
    "            - LongTensor(bs, slen)\n",
    "                <EOS> W1 W2 W3 <EOS> <PAD>\n",
    "                <EOS> W1 W2 W3   W4  <EOS>\n",
    "        `lengths`:\n",
    "            - LongTensor(bs) [5, 6]\n",
    "        `positions`:\n",
    "            - False, for regular \"arange\" positions (LM)\n",
    "            - True, to reset positions from the new generation (MT)\n",
    "        `langs`:\n",
    "            - must be None if the model only supports one language\n",
    "            - lang_id if only one language is involved (LM)\n",
    "            - (lang_id1, lang_id2) if two languages are involved (MT)\n",
    "        \"\"\"\n",
    "\n",
    "        # input batch\n",
    "        bs = len(src_len)\n",
    "        assert src_enc.size(0) == bs\n",
    "\n",
    "        # generated sentences\n",
    "        generated = src_len.new(max_len, bs)  # upcoming output\n",
    "        generated.fill_(self.pad_index)       # fill upcoming ouput with <PAD>\n",
    "        generated[0].fill_(self.eos_index)    # we use <EOS> for <BOS> everywhere\n",
    "\n",
    "        # positions\n",
    "        positions = src_len.new(max_len).long()\n",
    "        positions = torch.arange(max_len, out=positions).unsqueeze(1).expand(max_len, bs)\n",
    "\n",
    "        # language IDs\n",
    "        langs = src_len.new(max_len).long().fill_(tgt_lang_id)\n",
    "        langs = langs.unsqueeze(1).expand(max_len, bs)\n",
    "\n",
    "        # current position / max lengths / length of generated sentences / unfinished sentences\n",
    "        cur_len = 1\n",
    "        gen_len = src_len.clone().fill_(1)\n",
    "        unfinished_sents = src_len.clone().fill_(1)\n",
    "\n",
    "        # cache compute states\n",
    "        cache = {'slen': 0}\n",
    "\n",
    "        while cur_len < max_len:\n",
    "\n",
    "            # compute word scores\n",
    "            tensor = self.forward(\n",
    "                'fwd',\n",
    "                x=generated[:cur_len],\n",
    "                lengths=gen_len,\n",
    "                positions=positions[:cur_len],\n",
    "                langs=langs[:cur_len],\n",
    "                causal=True,\n",
    "                src_enc=src_enc,\n",
    "                src_len=src_len,\n",
    "                cache=cache\n",
    "            )\n",
    "            assert tensor.size() == (1, bs, self.dim), (cur_len, max_len, src_enc.size(), tensor.size(), (1, bs, self.dim))\n",
    "            tensor = tensor.data[-1, :, :].type_as(src_enc)  # (bs, dim)\n",
    "            scores = self.pred_layer.get_scores(tensor)      # (bs, n_words)\n",
    "\n",
    "            # select next words: sample or greedy\n",
    "            if sample_temperature is None:\n",
    "                next_words = torch.topk(scores, 1)[1].squeeze(1)\n",
    "            else:\n",
    "                next_words = torch.multinomial(F.softmax(scores / sample_temperature, dim=1), 1).squeeze(1)\n",
    "            assert next_words.size() == (bs,)\n",
    "\n",
    "            # update generations / lengths / finished sentences / current length\n",
    "            generated[cur_len] = next_words * unfinished_sents + self.pad_index * (1 - unfinished_sents)\n",
    "            gen_len.add_(unfinished_sents)\n",
    "            unfinished_sents.mul_(next_words.ne(self.eos_index).long())\n",
    "            cur_len = cur_len + 1\n",
    "\n",
    "            # stop when there is a </s> in each sentence, or if we exceed the maximul length\n",
    "            if unfinished_sents.max() == 0:\n",
    "                break\n",
    "\n",
    "        # add <EOS> to unfinished sentences\n",
    "        if cur_len == max_len:\n",
    "            generated[-1].masked_fill_(unfinished_sents.byte(), self.eos_index)\n",
    "\n",
    "        # sanity check\n",
    "        assert (generated == self.eos_index).sum() == 2 * bs\n",
    "\n",
    "        return generated[:cur_len], gen_len\n",
    "\n",
    "    def generate_beam(self, src_enc, src_len, tgt_lang_id, beam_size, length_penalty, early_stopping, max_len=200):\n",
    "        \"\"\"\n",
    "        Decode a sentence given initial start.\n",
    "        `x`:\n",
    "            - LongTensor(bs, slen)\n",
    "                <EOS> W1 W2 W3 <EOS> <PAD>\n",
    "                <EOS> W1 W2 W3   W4  <EOS>\n",
    "        `lengths`:\n",
    "            - LongTensor(bs) [5, 6]\n",
    "        `positions`:\n",
    "            - False, for regular \"arange\" positions (LM)\n",
    "            - True, to reset positions from the new generation (MT)\n",
    "        `langs`:\n",
    "            - must be None if the model only supports one language\n",
    "            - lang_id if only one language is involved (LM)\n",
    "            - (lang_id1, lang_id2) if two languages are involved (MT)\n",
    "        \"\"\"\n",
    "\n",
    "        # check inputs\n",
    "        assert src_enc.size(0) == src_len.size(0)\n",
    "        assert beam_size >= 1\n",
    "\n",
    "        # batch size / number of words\n",
    "        bs = len(src_len)\n",
    "        n_words = self.n_words\n",
    "\n",
    "        # expand to beam size the source latent representations / source lengths\n",
    "        src_enc = src_enc.unsqueeze(1).expand((bs, beam_size) + src_enc.shape[1:]).contiguous().view((bs * beam_size,) + src_enc.shape[1:])\n",
    "        src_len = src_len.unsqueeze(1).expand(bs, beam_size).contiguous().view(-1)\n",
    "\n",
    "        # generated sentences (batch with beam current hypotheses)\n",
    "        generated = src_len.new(max_len, bs * beam_size)  # upcoming output\n",
    "        generated.fill_(self.pad_index)                   # fill upcoming ouput with <PAD>\n",
    "        generated[0].fill_(self.eos_index)                # we use <EOS> for <BOS> everywhere\n",
    "\n",
    "        # generated hypotheses\n",
    "        generated_hyps = [BeamHypotheses(beam_size, max_len, length_penalty, early_stopping) for _ in range(bs)]\n",
    "\n",
    "        # positions\n",
    "        positions = src_len.new(max_len).long()\n",
    "        positions = torch.arange(max_len, out=positions).unsqueeze(1).expand_as(generated)\n",
    "\n",
    "        # language IDs\n",
    "        langs = positions.clone().fill_(tgt_lang_id)\n",
    "\n",
    "        # scores for each sentence in the beam\n",
    "        beam_scores = src_enc.new(bs, beam_size).fill_(0)\n",
    "        beam_scores[:, 1:] = -1e9\n",
    "        beam_scores = beam_scores.view(-1)\n",
    "\n",
    "        # current position\n",
    "        cur_len = 1\n",
    "\n",
    "        # cache compute states\n",
    "        cache = {'slen': 0}\n",
    "\n",
    "        # done sentences\n",
    "        done = [False for _ in range(bs)]\n",
    "\n",
    "        while cur_len < max_len:\n",
    "\n",
    "            # compute word scores\n",
    "            tensor = self.forward(\n",
    "                'fwd',\n",
    "                x=generated[:cur_len],\n",
    "                lengths=src_len.new(bs * beam_size).fill_(cur_len),\n",
    "                positions=positions[:cur_len],\n",
    "                langs=langs[:cur_len],\n",
    "                causal=True,\n",
    "                src_enc=src_enc,\n",
    "                src_len=src_len,\n",
    "                cache=cache\n",
    "            )\n",
    "            assert tensor.size() == (1, bs * beam_size, self.dim)\n",
    "            tensor = tensor.data[-1, :, :]               # (bs * beam_size, dim)\n",
    "            scores = self.pred_layer.get_scores(tensor)  # (bs * beam_size, n_words)\n",
    "            scores = F.log_softmax(scores, dim=-1)       # (bs * beam_size, n_words)\n",
    "            assert scores.size() == (bs * beam_size, n_words)\n",
    "\n",
    "            # select next words with scores\n",
    "            _scores = scores + beam_scores[:, None].expand_as(scores)  # (bs * beam_size, n_words)\n",
    "            _scores = _scores.view(bs, beam_size * n_words)            # (bs, beam_size * n_words)\n",
    "\n",
    "            next_scores, next_words = torch.topk(_scores, 2 * beam_size, dim=1, largest=True, sorted=True)\n",
    "            assert next_scores.size() == next_words.size() == (bs, 2 * beam_size)\n",
    "\n",
    "            # next batch beam content\n",
    "            # list of (bs * beam_size) tuple(next hypothesis score, next word, current position in the batch)\n",
    "            next_batch_beam = []\n",
    "\n",
    "            # for each sentence\n",
    "            for sent_id in range(bs):\n",
    "\n",
    "                # if we are done with this sentence\n",
    "                done[sent_id] = done[sent_id] or generated_hyps[sent_id].is_done(next_scores[sent_id].max().item())\n",
    "                if done[sent_id]:\n",
    "                    next_batch_beam.extend([(0, self.pad_index, 0)] * beam_size)  # pad the batch\n",
    "                    continue\n",
    "\n",
    "                # next sentence beam content\n",
    "                next_sent_beam = []\n",
    "\n",
    "                # next words for this sentence\n",
    "                for idx, value in zip(next_words[sent_id], next_scores[sent_id]):\n",
    "\n",
    "                    # get beam and word IDs\n",
    "                    beam_id = idx // n_words\n",
    "                    word_id = idx % n_words\n",
    "\n",
    "                    # end of sentence, or next word\n",
    "                    if word_id == self.eos_index or cur_len + 1 == max_len:\n",
    "                        generated_hyps[sent_id].add(generated[:cur_len, sent_id * beam_size + beam_id].clone(), value.item())\n",
    "                    else:\n",
    "                        next_sent_beam.append((value, word_id, sent_id * beam_size + beam_id))\n",
    "\n",
    "                    # the beam for next step is full\n",
    "                    if len(next_sent_beam) == beam_size:\n",
    "                        break\n",
    "\n",
    "                # update next beam content\n",
    "                assert len(next_sent_beam) == 0 if cur_len + 1 == max_len else beam_size\n",
    "                if len(next_sent_beam) == 0:\n",
    "                    next_sent_beam = [(0, self.pad_index, 0)] * beam_size  # pad the batch\n",
    "                next_batch_beam.extend(next_sent_beam)\n",
    "                assert len(next_batch_beam) == beam_size * (sent_id + 1)\n",
    "\n",
    "            # sanity check / prepare next batch\n",
    "            assert len(next_batch_beam) == bs * beam_size\n",
    "            beam_scores = beam_scores.new([x[0] for x in next_batch_beam])\n",
    "            beam_words = generated.new([x[1] for x in next_batch_beam])\n",
    "            beam_idx = src_len.new([x[2] for x in next_batch_beam])\n",
    "\n",
    "            # re-order batch and internal states\n",
    "            generated = generated[:, beam_idx]\n",
    "            generated[cur_len] = beam_words\n",
    "            for k in cache.keys():\n",
    "                if k != 'slen':\n",
    "                    cache[k] = (cache[k][0][beam_idx], cache[k][1][beam_idx])\n",
    "\n",
    "            # update current length\n",
    "            cur_len = cur_len + 1\n",
    "\n",
    "            # stop when we are done with each sentence\n",
    "            if all(done):\n",
    "                break\n",
    "\n",
    "        # visualize hypotheses\n",
    "        # print([len(x) for x in generated_hyps], cur_len)\n",
    "        # globals().update( locals() );\n",
    "        # !import code; code.interact(local=vars())\n",
    "        # for ii in range(bs):\n",
    "        #     for ss, ww in sorted(generated_hyps[ii].hyp, key=lambda x: x[0], reverse=True):\n",
    "        #         print(\"%.3f \" % ss + \" \".join(self.dico[x] for x in ww.tolist()))\n",
    "        #     print(\"\")\n",
    "\n",
    "        # select the best hypotheses\n",
    "        tgt_len = src_len.new(bs)\n",
    "        best = []\n",
    "\n",
    "        for i, hypotheses in enumerate(generated_hyps):\n",
    "            best_hyp = max(hypotheses.hyp, key=lambda x: x[0])[1]\n",
    "            tgt_len[i] = len(best_hyp) + 1  # +1 for the <EOS> symbol\n",
    "            best.append(best_hyp)\n",
    "\n",
    "        # generate target batch\n",
    "        decoded = src_len.new(tgt_len.max().item(), bs).fill_(self.pad_index)\n",
    "        for i, hypo in enumerate(best):\n",
    "            decoded[:tgt_len[i] - 1, i] = hypo\n",
    "            decoded[tgt_len[i] - 1, i] = self.eos_index\n",
    "\n",
    "        # sanity check\n",
    "        assert (decoded == self.eos_index).sum() == 2 * bs\n",
    "\n",
    "        return decoded, tgt_len\n",
    "\n",
    "\n",
    "class BeamHypotheses(object):\n",
    "\n",
    "    def __init__(self, n_hyp, max_len, length_penalty, early_stopping):\n",
    "        \"\"\"\n",
    "        Initialize n-best list of hypotheses.\n",
    "        \"\"\"\n",
    "        self.max_len = max_len - 1  # ignoring <BOS>\n",
    "        self.length_penalty = length_penalty\n",
    "        self.early_stopping = early_stopping\n",
    "        self.n_hyp = n_hyp\n",
    "        self.hyp = []\n",
    "        self.worst_score = 1e9\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Number of hypotheses in the list.\n",
    "        \"\"\"\n",
    "        return len(self.hyp)\n",
    "\n",
    "    def add(self, hyp, sum_logprobs):\n",
    "        \"\"\"\n",
    "        Add a new hypothesis to the list.\n",
    "        \"\"\"\n",
    "        score = sum_logprobs / len(hyp) ** self.length_penalty\n",
    "        if len(self) < self.n_hyp or score > self.worst_score:\n",
    "            self.hyp.append((score, hyp))\n",
    "            if len(self) > self.n_hyp:\n",
    "                sorted_scores = sorted([(s, idx) for idx, (s, _) in enumerate(self.hyp)])\n",
    "                del self.hyp[sorted_scores[0][1]]\n",
    "                self.worst_score = sorted_scores[1][0]\n",
    "            else:\n",
    "                self.worst_score = min(score, self.worst_score)\n",
    "\n",
    "    def is_done(self, best_sum_logprobs):\n",
    "        \"\"\"\n",
    "        If there are enough hypotheses and that none of the hypotheses being generated\n",
    "        can become better than the worst one in the heap, then we are done with this sentence.\n",
    "        \"\"\"\n",
    "        if len(self) < self.n_hyp:\n",
    "            return False\n",
    "        elif self.early_stopping:\n",
    "            return True\n",
    "        else:\n",
    "            return self.worst_score >= best_sum_logprobs / self.max_len ** self.length_penalty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30916d45",
   "metadata": {},
   "source": [
    "Footer\n",
    "© 2022 GitHub, Inc.\n",
    "Footer navigation\n",
    "Terms\n",
    "Privacy\n",
    "Security\n",
    "Status\n",
    "Docs\n",
    "Contact GitHub\n",
    "Pricing\n",
    "API\n",
    "Training\n",
    "Blog\n",
    "About\n",
    "You have no unread notifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17466113",
   "metadata": {},
   "outputs": [],
   "source": [
    "sin_emb = create_sinusoidal_embeddings(2, 2, torch.tensor([[24,24],[24,24]] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "77c058f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#params, dico, is_encoder, with_output\n",
    "class Params:\n",
    "    def __init__(self, n_langs, n_words, eos_index, pad_idx, id2lang, lang2id, emb_dim, n_heads, n_layers, dropout, attention_dropout):\n",
    "        self.n_langs = n_langs\n",
    "        self.n_words = n_words\n",
    "        self.eos_index = eos_index\n",
    "        self.pad_index = pad_idx\n",
    "        self.id2lang = id2lang\n",
    "        self.lang2id = lang2id\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.sinusoidal_embeddings = create_sinusoidal_embeddings(2, 2, torch.tensor([[24,24],[24,24]] ))\n",
    "        self.gelu_activation = gelu(torch.tensor([2]))\n",
    "        self.asm = 1\n",
    "        self.asm_cutoffs = torch.tensor([1,2,3])\n",
    "        self.asm_div_value = torch.tensor([1])\n",
    "        self.share_inout_emb = True\n",
    "        \n",
    "n_langs = 2\n",
    "n_words = 10000\n",
    "params = Params(n_langs, n_words, 500, 40, np.ones(n_langs), np.ones(n_langs), 512,8, 6, 0.1, 0.1)\n",
    "dico = np.ones(n_words)\n",
    "\n",
    "model = TransformerModel(params, dico, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a06159a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['position_embeddings.weight',\n",
       " 'lang_embeddings.weight',\n",
       " 'embeddings.weight',\n",
       " 'layer_norm_emb.weight',\n",
       " 'layer_norm_emb.bias',\n",
       " 'attentions.0.q_lin.weight',\n",
       " 'attentions.0.q_lin.bias',\n",
       " 'attentions.0.k_lin.weight',\n",
       " 'attentions.0.k_lin.bias',\n",
       " 'attentions.0.v_lin.weight',\n",
       " 'attentions.0.v_lin.bias',\n",
       " 'attentions.0.out_lin.weight',\n",
       " 'attentions.0.out_lin.bias',\n",
       " 'attentions.1.q_lin.weight',\n",
       " 'attentions.1.q_lin.bias',\n",
       " 'attentions.1.k_lin.weight',\n",
       " 'attentions.1.k_lin.bias',\n",
       " 'attentions.1.v_lin.weight',\n",
       " 'attentions.1.v_lin.bias',\n",
       " 'attentions.1.out_lin.weight',\n",
       " 'attentions.1.out_lin.bias',\n",
       " 'attentions.2.q_lin.weight',\n",
       " 'attentions.2.q_lin.bias',\n",
       " 'attentions.2.k_lin.weight',\n",
       " 'attentions.2.k_lin.bias',\n",
       " 'attentions.2.v_lin.weight',\n",
       " 'attentions.2.v_lin.bias',\n",
       " 'attentions.2.out_lin.weight',\n",
       " 'attentions.2.out_lin.bias',\n",
       " 'attentions.3.q_lin.weight',\n",
       " 'attentions.3.q_lin.bias',\n",
       " 'attentions.3.k_lin.weight',\n",
       " 'attentions.3.k_lin.bias',\n",
       " 'attentions.3.v_lin.weight',\n",
       " 'attentions.3.v_lin.bias',\n",
       " 'attentions.3.out_lin.weight',\n",
       " 'attentions.3.out_lin.bias',\n",
       " 'attentions.4.q_lin.weight',\n",
       " 'attentions.4.q_lin.bias',\n",
       " 'attentions.4.k_lin.weight',\n",
       " 'attentions.4.k_lin.bias',\n",
       " 'attentions.4.v_lin.weight',\n",
       " 'attentions.4.v_lin.bias',\n",
       " 'attentions.4.out_lin.weight',\n",
       " 'attentions.4.out_lin.bias',\n",
       " 'attentions.5.q_lin.weight',\n",
       " 'attentions.5.q_lin.bias',\n",
       " 'attentions.5.k_lin.weight',\n",
       " 'attentions.5.k_lin.bias',\n",
       " 'attentions.5.v_lin.weight',\n",
       " 'attentions.5.v_lin.bias',\n",
       " 'attentions.5.out_lin.weight',\n",
       " 'attentions.5.out_lin.bias',\n",
       " 'layer_norm1.0.weight',\n",
       " 'layer_norm1.0.bias',\n",
       " 'layer_norm1.1.weight',\n",
       " 'layer_norm1.1.bias',\n",
       " 'layer_norm1.2.weight',\n",
       " 'layer_norm1.2.bias',\n",
       " 'layer_norm1.3.weight',\n",
       " 'layer_norm1.3.bias',\n",
       " 'layer_norm1.4.weight',\n",
       " 'layer_norm1.4.bias',\n",
       " 'layer_norm1.5.weight',\n",
       " 'layer_norm1.5.bias',\n",
       " 'ffns.0.lin1.weight',\n",
       " 'ffns.0.lin1.bias',\n",
       " 'ffns.0.lin2.weight',\n",
       " 'ffns.0.lin2.bias',\n",
       " 'ffns.1.lin1.weight',\n",
       " 'ffns.1.lin1.bias',\n",
       " 'ffns.1.lin2.weight',\n",
       " 'ffns.1.lin2.bias',\n",
       " 'ffns.2.lin1.weight',\n",
       " 'ffns.2.lin1.bias',\n",
       " 'ffns.2.lin2.weight',\n",
       " 'ffns.2.lin2.bias',\n",
       " 'ffns.3.lin1.weight',\n",
       " 'ffns.3.lin1.bias',\n",
       " 'ffns.3.lin2.weight',\n",
       " 'ffns.3.lin2.bias',\n",
       " 'ffns.4.lin1.weight',\n",
       " 'ffns.4.lin1.bias',\n",
       " 'ffns.4.lin2.weight',\n",
       " 'ffns.4.lin2.bias',\n",
       " 'ffns.5.lin1.weight',\n",
       " 'ffns.5.lin1.bias',\n",
       " 'ffns.5.lin2.weight',\n",
       " 'ffns.5.lin2.bias',\n",
       " 'layer_norm2.0.weight',\n",
       " 'layer_norm2.0.bias',\n",
       " 'layer_norm2.1.weight',\n",
       " 'layer_norm2.1.bias',\n",
       " 'layer_norm2.2.weight',\n",
       " 'layer_norm2.2.bias',\n",
       " 'layer_norm2.3.weight',\n",
       " 'layer_norm2.3.bias',\n",
       " 'layer_norm2.4.weight',\n",
       " 'layer_norm2.4.bias',\n",
       " 'layer_norm2.5.weight',\n",
       " 'layer_norm2.5.bias',\n",
       " 'pred_layer.proj.weight',\n",
       " 'pred_layer.proj.head.weight',\n",
       " 'pred_layer.proj.head.bias',\n",
       " 'pred_layer.proj.tail.0.0.weight',\n",
       " 'pred_layer.proj.tail.0.1.weight',\n",
       " 'pred_layer.proj.tail.1.0.weight',\n",
       " 'pred_layer.proj.tail.1.1.weight',\n",
       " 'pred_layer.proj.tail.2.0.weight',\n",
       " 'pred_layer.proj.tail.2.1.weight']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_keys = list(model.state_dict().keys())\n",
    "m_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "08e7d3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2529a86a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
