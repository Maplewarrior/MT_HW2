{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sacremoses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnVuij-35_1n",
        "outputId": "39936d8b-75a4-44c5-c812-7cbed3fc3cdb"
      },
      "id": "MnVuij-35_1n",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (0.0.53)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "7bff3a26",
      "metadata": {
        "id": "7bff3a26"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import XLMTokenizer, XLMWithLMHeadModel, AutoTokenizer\n",
        "import random\n",
        "import copy\n",
        "import time\n",
        "import re\n",
        "import os.path as osp\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ca302927",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca302927",
        "outputId": "a2210e3c-7a22-4582-97f6-d006cdcd538d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMWithLMHeadModel were not initialized from the model checkpoint at xlm-mlm-enfr-1024 and are newly initialized: ['transformer.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_checkpoint = 'xlm-mlm-enfr-1024'\n",
        "tokenizer = XLMTokenizer.from_pretrained(model_checkpoint)\n",
        "pre_model = XLMWithLMHeadModel.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAqWtFJq6zYI",
        "outputId": "64ddd17d-d048-4721-c93a-9978e4f46aac"
      },
      "id": "XAqWtFJq6zYI",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a051419d",
      "metadata": {
        "id": "a051419d"
      },
      "source": [
        "# Load data here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFF5s-Cj7GBc",
        "outputId": "50e52092-4ff6-4eba-ddf7-53e182e068f7"
      },
      "id": "CFF5s-Cj7GBc",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f01ea585",
      "metadata": {
        "id": "f01ea585"
      },
      "outputs": [],
      "source": [
        "data_en = open(\"/content/drive/MyDrive/Colab Notebooks/mt_hw2/hansards.e\", encoding='utf-8').read().split('\\n')\n",
        "data_fr = open(\"/content/drive/MyDrive/Colab Notebooks/mt_hw2/hansards.f\", encoding='utf-8').read().split('\\n')\n",
        "\n",
        "\n",
        "raw_data = {'en': [line for line in data_en], 'fr': [line for line in data_fr]}\n",
        "\n",
        "df = pd.DataFrame(raw_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ba563b",
      "metadata": {
        "id": "22ba563b"
      },
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "92cda094",
      "metadata": {
        "id": "92cda094"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Tokenizer\n",
        "#model_checkpoint = 't5-small'\n",
        "#tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, model_max_length=512)\n",
        "\n",
        "# Function for mapping data from strings to tokens\n",
        "# s_key = source key, t_key = target_key, max_length = max sentence length\n",
        "def preprocess_data(df, s_key, t_key, max_length):\n",
        "    s = [sentence for sentence in df[s_key]]\n",
        "    t = [sentence for sentence in df[t_key]]\n",
        "    \n",
        "    model_input = tokenizer(s, max_length=max_length, truncation=True, padding=True, return_tensors='pt') \n",
        "    \n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        target_tokens = tokenizer(t, truncation=True, padding=True, max_length=max_length, return_tensors='pt') \n",
        "        \n",
        "    model_input['target'] = target_tokens['input_ids']\n",
        "   \n",
        "    return model_input\n",
        "\"\"\"\n",
        "def tokenize_data(texts, tokenizer, maxlen=512):\n",
        "    enc_di = tokenizer.batch_encode_plus(\n",
        "        texts, \n",
        "        return_token_type_ids=False,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors='pt',\n",
        "        max_length=maxlen\n",
        "    )\n",
        "    \n",
        "    return enc_di\n",
        "\n",
        "\n",
        "# Tokenize data\n",
        "en_data = tokenize_data(df['en'], tokenizer)\n",
        "fr_data = tokenize_data(df['fr'], tokenizer)\n",
        "\n",
        "# Function for mapping data from strings to tokens\n",
        "# s_key = source key, t_key = target_key, max_length = max sentence length\n",
        "def preprocess_data(df, s_key, t_key, max_length):\n",
        "    df = df[df[s_key].apply(lambda x: isinstance(x, str))]\n",
        "    df = df[df[t_key].apply(lambda x: isinstance(x, str))]\n",
        "\n",
        "    df[s_key] = df[s_key].map(pad_punct)\n",
        "    df[t_key] = df[t_key].map(pad_punct)\n",
        "\n",
        "    s = list(df[s_key].values)\n",
        "    t = list(df[t_key].values)\n",
        "    \n",
        "    model_input = tokenizer(s, max_length=max_length, truncation=True, padding=True, return_tensors='pt') \n",
        "    \n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        target_tokens = tokenizer(t, truncation=True, padding=True, max_length=max_length, return_tensors='pt') \n",
        "        \n",
        "    model_input['target'] = target_tokens['input_ids']\n",
        "    \n",
        "    return model_input, df\n",
        "\n",
        "def pad_punct(s):\n",
        "    s = re.sub('([.,!?()])', r' \\1 ', s)\n",
        "    return re.sub('\\s{2,}', ' ', s)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EnFrDataset(Dataset):\n",
        "    def __init__(self, df, src_col='en', trg_col='fr'):\n",
        "        self.df = df\n",
        "        self.src_col = src_col\n",
        "        self.trg_col = trg_col\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source = self.df[self.src_col][idx]\n",
        "        target = self.df[self.trg_col][idx]\n",
        "        return source, target"
      ],
      "metadata": {
        "id": "aDuBrvMk-8pK"
      },
      "id": "aDuBrvMk-8pK",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d326aaa2",
      "metadata": {
        "id": "d326aaa2"
      },
      "outputs": [],
      "source": [
        "# Example of tokenize output\n",
        "#print(en_data['input_ids'][1])\n",
        "#print(df_small['en'][1]) # \"the\" occures on idx 0\n",
        "#print(df_small['en'][2]) # \"the\" occures on idx 13\n",
        "#print(en_data['input_ids'][2])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb7cccc1",
      "metadata": {
        "id": "cb7cccc1"
      },
      "source": [
        "## All transformer classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2b4e94d0",
      "metadata": {
        "id": "2b4e94d0"
      },
      "outputs": [],
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n",
        "\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len=512):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # Allocate memory to \n",
        "        pe = torch.zeros((max_seq_len, d_model))\n",
        "        \n",
        "        ### From attention is all you need ###\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0,d_model,2):\n",
        "                pe[pos, i] = np.sin(pos/10000**(2*i/self.d_model))\n",
        "                pe[pos, i+1] = np.cos(pos/10000**(2*i/self.d_model))\n",
        "        # Fixed positional encoding\n",
        "        pe.requires_grad = False\n",
        "        #pe = pe.unsqueeze(0) # Make pe into [batch size x seq_len x d_model]\n",
        "        self.register_buffer('pe',pe)\n",
        "        \n",
        "    def forward(self,x):\n",
        "\n",
        "        # Make embeddings larger\n",
        "        x = x*np.sqrt(self.d_model)\n",
        "        # Get sequence length\n",
        "        seq_len = x.size(1)\n",
        "        \n",
        "        pe = self.pe.clone()\n",
        "        pe = pe.unsqueeze(0)\n",
        "        \n",
        "        x = x + torch.autograd.Variable(pe[:,:seq_len], \n",
        "        requires_grad=False)\n",
        "        return x\n",
        "\n",
        "\n",
        "def Attention(Q, K, V, d_k, mask=None, dropout=None):\n",
        "\n",
        "    vals = (Q @ K.transpose(-2,-1))/np.sqrt(d_k)\n",
        "    \n",
        "    # Mask the scores if mask is specified. Model cannot see into future if masked.\n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1)\n",
        "        vals = vals.masked_fill(mask, 1e-9)\n",
        "    # vals = vals if mask is None else vals.masked_fill_(mask, 1e-4)\n",
        "    \n",
        "    softmax = nn.Softmax(dim=-1)\n",
        "    vals = softmax(vals)\n",
        "    \n",
        "    # apply dropout if specified\n",
        "    vals = vals if dropout is None else dropout(vals)\n",
        "    \n",
        "    out =  vals @ V\n",
        "    return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, d_k, dropout=.1, relative = False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        # self.seq_len = seq_len\n",
        "        self.d_k = d_k\n",
        "        \n",
        "        self.linearQ = nn.Linear(d_model, d_model)\n",
        "        self.linearK = nn.Linear(d_model, d_model)\n",
        "        self.linearV = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    # d_model = 512\n",
        "    # n_heads = 8\n",
        "    # seq_len = 20\n",
        "    \n",
        "    # [20,512] --> [20, 8, 64]\n",
        "    ## If batch size is used, say of 128:\n",
        "    ## out = [128, 20, 8, 64]\n",
        "    \n",
        "    # Input = Matrix of dim [bs x seq_len x d_model]\n",
        "    def split_heads(self, t):\n",
        "        return t.reshape(t.size(0), -1, self.n_heads, int(self.d_k))\n",
        "    # Output = Matrix of dim [bs x seq_len x n_heads x d_k]\n",
        "    \n",
        "    def forward(self, Q, K, V, mask = None):\n",
        "        \n",
        "        Q = self.linearQ(Q)\n",
        "        K = self.linearK(K)\n",
        "        V = self.linearV(V)\n",
        "        \n",
        "        Q, K, V = [self.split_heads(t) for t in (Q, K, V)] \n",
        "        Q, K, V = [t.transpose(1,2) for t in (Q, K, V)] # reshape to [bs x n_heads x seq_len x d_k]\n",
        "        \n",
        "        # Compute Attention\n",
        "        vals = Attention(Q, K, V, self.d_k, mask, self.dropout)\n",
        "        \n",
        "        # Reshape to [bs x seq_len x d_model]\n",
        "        vals = vals.transpose(1,2).contiguous().view(vals.size(0), -1, self.d_model)\n",
        "       \n",
        "        out = self.out(vals) # linear\n",
        "        return out\n",
        "    \n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
        "    \n",
        "\n",
        "        \n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, d_ff, d_k, dropout=.1):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(n_heads, d_model, d_k, dropout)\n",
        "        self.ffns = FeedForwardNetwork(d_model, d_ff, dropout)\n",
        "        \n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        # See \"Attention is all you need\" to follow code structure\n",
        "        x2 = self.dropout1(self.attention(x, x, x, mask))\n",
        "        x = self.layer_norm1(x) + self.layer_norm1(x2)\n",
        "        \n",
        "        x2 = self.dropout2(self.ffns(x))\n",
        "        x = x + self.layer_norm2(x2)\n",
        "    \n",
        "        return x\n",
        "    \n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, d_ff, d_k, dropout=.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        \n",
        "        self.attention = MultiHeadAttention(n_heads, d_model, d_k, dropout)\n",
        "        self.ffns = FeedForwardNetwork(d_model, d_ff, dropout)\n",
        "        \n",
        "        # Batch Normalization\n",
        "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
        "        \n",
        "        # Dropout\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "        \n",
        "        # self.linear = nn.Linear()\n",
        "        \n",
        "    def forward(self, x, e_out, source_mask, target_mask):\n",
        "        \n",
        "        # See \"Attention is all you need\" to follow code structure\n",
        "        ## part 1\n",
        "        x2 = self.layer_norm1(x) # Norm\n",
        "        x = self.dropout1(self.attention.forward(x2, x2, x2, target_mask)) # Masked MHA, target\n",
        "        x = x2 + self.layer_norm1(x) # Add & Norm\n",
        "        \n",
        "        ## part 2\n",
        "        x3 = self.dropout2(self.attention.forward(x, e_out, e_out, source_mask)) # MHA on encoder output\n",
        "        x2 = self.dropout2(self.attention.forward(x, x, x)) #MHA continued in decoder\n",
        "        x = self.layer_norm1(x3) + self.layer_norm1(x2) + self.layer_norm1(x) # Add & Norm\n",
        "        \n",
        "        ## part 3\n",
        "        x2 = self.dropout3(self.ffns.forward(x)) ## Feed forward\n",
        "        x = x + self.layer_norm2(x2) # add\n",
        "        # x = self.norm3(x) # norm (!!!CHECK IF THIS IS EQUIVALENT!!!)\n",
        "        return x\n",
        "\n",
        "def cloneLayers(module, n_layers):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(n_layers)])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, d_ff, d_k, n_layers, n_heads, dropout=.1):\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.embedder = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.e_layers = cloneLayers(EncoderLayer(n_heads, d_model, d_ff, d_k), n_layers)\n",
        "        \n",
        "    def forward(self, source, mask=None):\n",
        "        x = self.embedder.forward(source)\n",
        "        x = self.pe.forward(x)\n",
        "        for i in range(self.n_layers):\n",
        "            x = self.e_layers[i](x, mask)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, d_ff, d_k, n_layers, n_heads, dropout=.1):\n",
        "        super().__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.embedder = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.d_layers = cloneLayers(DecoderLayer(n_heads, d_model, d_ff, d_k), n_layers)\n",
        "        \n",
        "    \n",
        "    def forward(self, trg, e_out, source_mask, target_mask):\n",
        "        x = self.embedder.forward(trg)\n",
        "        x = self.pe.forward(x)\n",
        "        \n",
        "        for i in range(self.n_layers):\n",
        "            x = self.d_layers[i](x, e_out, source_mask, target_mask)\n",
        "        \n",
        "        return x\n",
        "        \n",
        "class AlignmentLayer(nn.Module):\n",
        "    def __init__(self, source_vocab_size, target_vocab_size, d_model, d_ff, d_k, n_layers, n_heads):\n",
        "        super().__init__()\n",
        "        self.MHA = MultiHeadAttention(n_heads, d_model, d_k)\n",
        "        self.e = Encoder(source_vocab_size, d_model,d_ff, d_k, n_layers, n_heads)\n",
        "        self.d = Decoder(target_vocab_size, d_model,d_ff, d_k, n_layers, n_heads)\n",
        "        self.linear_f = nn.Linear(d_model, target_vocab_size) \n",
        "        \n",
        "    def forward(self, source, target, source_mask, target_mask):\n",
        "        e_out = self.e.forward(source, source_mask)\n",
        "        d_out = self.d.forward(target, e_out, source_mask, target_mask)\n",
        "        \n",
        "        out = self.MHA.forward(d_out, e_out, e_out)\n",
        "        \n",
        "        \n",
        "        \n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, source_vocab_size, target_vocab_size, d_model,d_ff, d_k, n_layers, n_heads):\n",
        "        super().__init__()\n",
        "        self.e = Encoder(source_vocab_size, d_model,d_ff, d_k, n_layers, n_heads)\n",
        "        self.d = Decoder(target_vocab_size, d_model,d_ff, d_k, n_layers, n_heads)\n",
        "        self.linear_f = nn.Linear(d_model, target_vocab_size)\n",
        "        \n",
        "    def forward(self, source, target, source_mask, target_mask):\n",
        "        e_out = self.e.forward(source, source_mask)\n",
        "        d_out = self.d.forward(target, e_out, source_mask, target_mask)\n",
        "        \n",
        "        out = self.linear_f(d_out)\n",
        "        return out\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f1e3f488",
      "metadata": {
        "id": "f1e3f488"
      },
      "outputs": [],
      "source": [
        "def get_target_mask(size, target):\n",
        "    mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float(0)).masked_fill(mask == 1, float(1))\n",
        "    \n",
        "    # Find out where the target pad starts\n",
        "    trg_pad = (target==0).nonzero()\n",
        "    \n",
        "    # Check if there is no padding in sentence\n",
        "    if len(trg_pad) == 0:\n",
        "        stop_idx = size\n",
        "        \n",
        "    else:\n",
        "        stop_idx = trg_pad[0][1].item()\n",
        "        mask[stop_idx:, :] = -69\n",
        "        \n",
        "    return mask.unsqueeze(0) > 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3e7abf80",
      "metadata": {
        "id": "3e7abf80"
      },
      "outputs": [],
      "source": [
        "def get_source_mask(size, source):\n",
        "    src_pad = (source==2).nonzero()\n",
        "    \n",
        "    if len(src_pad) == 0:\n",
        "        stop_idx = size\n",
        "        \n",
        "    else:\n",
        "        stop_idx = src_pad[0][1].item()\n",
        "        \n",
        "    mask = source.clone()\n",
        "    # Mask all padding with -inf\n",
        "    mask[:,stop_idx:] = 1\n",
        "    # Convert everything before stop_idx to zero\n",
        "    mask[:,:stop_idx] = 0\n",
        "    \n",
        "    mask = mask.unsqueeze(0) > 0\n",
        "    \n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a1d98edf",
      "metadata": {
        "id": "a1d98edf"
      },
      "outputs": [],
      "source": [
        "#print(\"Testing masking function for new tokenizer...\")\n",
        "\n",
        "#en1 = en_data['input_ids'][1]\n",
        "#en1.size()\n",
        "#pad = (en1==2).nonzero()\n",
        "#pad[0].item()\n",
        "#en1_msk = get_source_mask(en1.size(), en1.unsqueeze(0))\n",
        "#en1_new = en1\n",
        "\n",
        "\n",
        "#print(\"source:\\n\", en1)\n",
        "#en1_new = en1_new.masked_fill(en1_msk, 1e-5)\n",
        "#print(\"mask:\\n\", en1_msk)\n",
        "#en1_new = en1_new.masked_fill(en1_msk, -1)\n",
        "#print(\"after masking:\\n\", en1_new)\n",
        "\n",
        "\n",
        "#fr1 = fr_data['input_ids'][1]\n",
        "#fr1_msk = get_source_mask(fr1.size(), fr1.unsqueeze(0))\n",
        "#fr1_new = fr1\n",
        "\n",
        "#fr1_new = fr1_new.masked_fill(fr1_msk, -1)\n",
        "#fr1_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "483253c7",
      "metadata": {
        "scrolled": true,
        "id": "483253c7"
      },
      "source": [
        "# Define train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "0910e650",
      "metadata": {
        "id": "0910e650"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def train_model(model, dataloader, epochs, print_every=1e3, verbose=True):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    start = time.time()\n",
        "    tot_time = 0\n",
        "    total_loss = 0\n",
        "    loss_list = []\n",
        "    \n",
        "    \n",
        "    num_sentences = len(dataloader)\n",
        "    # loop over epochs\n",
        "    for epoch in range(epochs):\n",
        "        print(\"\\nEPOCH \" +str(epoch+1)+\" of \"+str(epochs)+\"\\n\")\n",
        "        # loop over all sentences\n",
        "        for i, (src, target) in enumerate(dataloader):        \n",
        "            print(src.size())    \n",
        "            # unsqueeze to avoid dim mismatch between embedder and pe\n",
        "            # src = src.unsqueeze(0).to(device)\n",
        "            # trg = src.unsqueeze(0).to(device)\n",
        "            # # target input, remove last word\n",
        "            # trg_input = trg[:, :-1]\n",
        "            \n",
        "            # # get targets\n",
        "            # y = trg[:, 1:].contiguous().view(-1)\n",
        "            \n",
        "            # src_mask = get_source_mask(src.size(1), src).to(device)\n",
        "            # trg_mask = get_target_mask(trg_input.size(1), trg_input).to(device)\n",
        "            \n",
        "            # preds = model.forward(src, trg_input, src_mask, trg_mask)\n",
        "            # optim.zero_grad()    \n",
        "            # loss = F.cross_entropy(preds.view(-1, preds.size(-1)), y)\n",
        "            # loss.backward()\n",
        "            # optim.step()\n",
        "            # total_loss += loss.item()\n",
        "            if verbose and i % print_every == 0:\n",
        "                print(\"sentence:\\t\",i+1,\"\\ntime per batch:\\t\",np.round(time.time()-start, 2)/(i+1), \"\\nloss:\\t\", np.round(loss.item(),2), \"\\naverage loss:\\t\", np.round(total_loss,2)/(i+1)) \n",
        "        loss_list.append(total_loss/num_sentences)\n",
        "        scheduler.step()\n",
        "        end = time.time()\n",
        "        elapsed = end-start\n",
        "        tot_time += elapsed\n",
        "        avg_time = tot_time/(epoch+1)\n",
        "        est_remain = epochs*avg_time - tot_time\n",
        "        print(f'Train Loss: {loss_list[epoch]:.4f}')\n",
        "        print(f'Epoch took {elapsed:.1f}s')\n",
        "        print(f'Estimated {(est_remain/60):.1f}m left')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "695fb80e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "695fb80e",
        "outputId": "5f6b9511-fef2-45dd-c699-f266d907b208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!!REMEMBER TO CHANGE EPOCHS!!!\n"
          ]
        }
      ],
      "source": [
        "### Define arguments ### (same as in \"Attention is all you need\")\n",
        "d_model = 1024 # Dimension of embeddings\n",
        "n_heads = 8 # Number of heads for MHA\n",
        "d_k = d_model/n_heads # dimension of keys (d_model / n_heads)\n",
        "d_ff = d_model*4 # DON'T CHANGE!!! (be careful)\n",
        "\n",
        "n_layers = 6 # Number of model layers\n",
        "epochs = 3\n",
        "\n",
        "src_vocab = set()\n",
        "trg_vocab = set()\n",
        "df['en'].str.split().apply(src_vocab.update)\n",
        "df['fr'].str.split().apply(trg_vocab.update)\n",
        "\n",
        "\n",
        "src_vocab_size = len(src_vocab)\n",
        "trg_vocab_size = len(trg_vocab)\n",
        "vocab_size = src_vocab_size + trg_vocab_size\n",
        "\n",
        "vocab_size = vocab_size # Number of (unique) words in dataset\n",
        "print(\"!!!REMEMBER TO CHANGE EPOCHS!!!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc69479c",
      "metadata": {
        "id": "bc69479c"
      },
      "source": [
        "# Define model and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "bb6d8054",
      "metadata": {
        "id": "bb6d8054"
      },
      "outputs": [],
      "source": [
        "pre_vocab_size = 64139\n",
        "model = Transformer(pre_vocab_size, pre_vocab_size, d_model, d_ff, d_k, n_layers, n_heads)\n",
        "\n",
        "lr = 0.00001 # 0.0001 default in \"AIAYN\"\n",
        "optim = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=10, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0117ad93",
      "metadata": {
        "id": "0117ad93"
      },
      "source": [
        "# Get pre-trained model weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "4d5b50bc",
      "metadata": {
        "scrolled": false,
        "id": "4d5b50bc"
      },
      "outputs": [],
      "source": [
        "#print(\"before weight initialization:\")\n",
        "#print(model.state_dict().values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "172013d2",
      "metadata": {
        "id": "172013d2"
      },
      "outputs": [],
      "source": [
        "pre_model_weights = pre_model.state_dict()\n",
        "sd = model.state_dict().copy()\n",
        "model_weights = model.state_dict()\n",
        "\n",
        "count = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for j in ['e.e_layers', 'd.d_layers']:\n",
        "        for i in range(n_layers):\n",
        "            # Embedding and Positional Encoding\n",
        "            sd['e.embedder.embed.weight'] = pre_model_weights['transformer.embeddings.weight']\n",
        "            sd['e.pe.pe'] = pre_model_weights['transformer.position_embeddings.weight']\n",
        "\n",
        "            # Attention layers\n",
        "            sd[f'{j}.{i}.attention.linearQ.weight']  = pre_model_weights[f'transformer.attentions.{i}.q_lin.weight']\n",
        "            sd[f'{j}.{i}.attention.linearQ.bias']    = pre_model_weights[f'transformer.attentions.{i}.q_lin.bias']\n",
        "            sd[f'{j}.{i}.attention.linearK.weight']  = pre_model_weights[f'transformer.attentions.{i}.k_lin.weight']\n",
        "            sd[f'{j}.{i}.attention.linearK.bias']    = pre_model_weights[f'transformer.attentions.{i}.k_lin.bias']\n",
        "            sd[f'{j}.{i}.attention.linearV.weight']  = pre_model_weights[f'transformer.attentions.{i}.v_lin.weight']\n",
        "            sd[f'{j}.{i}.attention.linearV.bias']    = pre_model_weights[f'transformer.attentions.{i}.v_lin.bias']\n",
        "            sd[f'{j}.{i}.attention.out.weight']   = pre_model_weights[f'transformer.attentions.{i}.out_lin.weight']\n",
        "            sd[f'{j}.{i}.attention.out.bias']     = pre_model_weights[f'transformer.attentions.{i}.out_lin.bias']\n",
        "\n",
        "            # Feed forwards\n",
        "            sd[f'{j}.{i}.ffns.linear1.weight'] = pre_model_weights[f'transformer.ffns.{i}.lin1.weight']\n",
        "            sd[f'{j}.{i}.ffns.linear1.bias'] = pre_model_weights[f'transformer.ffns.{i}.lin1.bias']\n",
        "            sd[f'{j}.{i}.ffns.linear2.weight'] = pre_model_weights[f'transformer.ffns.{i}.lin2.weight']\n",
        "            sd[f'{j}.{i}.ffns.linear2.bias'] = pre_model_weights[f'transformer.ffns.{i}.lin2.bias']\n",
        "\n",
        "            # Layer_norm1 = attention\n",
        "            sd[f'{j}.{i}.layer_norm1.weight'] = pre_model_weights[f'transformer.layer_norm1.{i}.weight']\n",
        "            sd[f'{j}.{i}.layer_norm1.bias'] = pre_model_weights[f'transformer.layer_norm1.{i}.bias']\n",
        "\n",
        "            #Layer_norm2 = FFN\n",
        "            sd[f'{j}.{i}.layer_norm2.weight'] = pre_model_weights[f'transformer.layer_norm2.{i}.weight']\n",
        "            sd[f'{j}.{i}.layer_norm2.bias'] = pre_model_weights[f'transformer.layer_norm2.{i}.bias']\n",
        "\n",
        "            # prediction layer\n",
        "            sd['linear_f.weight'] = pre_model_weights['pred_layer.proj.weight']\n",
        "            sd['linear_f.bias'] = pre_model_weights['pred_layer.proj.bias']\n",
        "            \n",
        "            \n",
        "            # fix\n",
        "            count += 1\n",
        "            if count >= len(list(model.state_dict().keys())):\n",
        "                break\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "4bb64e10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bb64e10",
        "outputId": "e46b6e98-6032-4c0f-b3c1-4c56f4e93d52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "model.load_state_dict(sd)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30cb4beb",
      "metadata": {
        "id": "30cb4beb"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = EnFrDataset(df)\n",
        "dataloader = Dataloader(dataset, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38KxGE3wAkk5",
        "outputId": "71b58c31-a7a4-4c61-bfc3-e0a49409840b"
      },
      "id": "38KxGE3wAkk5",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100001"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "489250c1",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "489250c1",
        "outputId": "47b84cba-9c8d-49d0-c40e-f4d396196fdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 1 of 3\n",
            "\n",
            "sentence:\t 1 \n",
            "time per batch:\t 0.15 \n",
            "loss:\t 8.67 \n",
            "average loss:\t 8.67\n",
            "sentence:\t 1001 \n",
            "time per batch:\t 0.13882117882117884 \n",
            "loss:\t 2.12 \n",
            "average loss:\t 3.5376823176823176\n",
            "sentence:\t 2001 \n",
            "time per batch:\t 0.13877561219390305 \n",
            "loss:\t 2.32 \n",
            "average loss:\t 2.826416791604198\n",
            "sentence:\t 3001 \n",
            "time per batch:\t 0.138757080973009 \n",
            "loss:\t 0.92 \n",
            "average loss:\t 2.5186604465178273\n",
            "sentence:\t 4001 \n",
            "time per batch:\t 0.13875281179705073 \n",
            "loss:\t 2.54 \n",
            "average loss:\t 2.319162709322669\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-8067be8dc6e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfr_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-57-1fb816613635>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, source_data, target_data, epochs, print_every, verbose)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_model(model, dataloader, epochs, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fedf012d",
      "metadata": {
        "id": "fedf012d"
      },
      "outputs": [],
      "source": [
        "filename = 'drive/MyDrive/Colab Notebooks/mt_hw2/EN-FR_translation_weights.pt'\n",
        "torch.save(model.state_dict(), filename)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}